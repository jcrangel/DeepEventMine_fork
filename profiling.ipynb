{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 14:15:39.478 | INFO     | bert.modeling:<module>:231 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "from eval.evaluate import predict\n",
    "\n",
    "from nets import deepEM\n",
    "from loader.prepData import prepdata\n",
    "from loader.prepNN import prep4nn\n",
    "from utils import utils\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_test_data(test_data, params):\n",
    "    test = prep4nn.data2network(test_data, 'predict', params)\n",
    "\n",
    "    if len(test) == 0:\n",
    "        raise ValueError(\"Test set empty.\")\n",
    "\n",
    "    test_data = prep4nn.torch_data_2_network(\n",
    "        cdata2network=test, params=params, do_get_nn_data=True)\n",
    "    te_data_size = len(test_data['nn_data']['ids'])\n",
    "\n",
    "    test_data_ids = TensorDataset(torch.arange(te_data_size))\n",
    "    test_sampler = SequentialSampler(test_data_ids)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data_ids, sampler=test_sampler, batch_size=params['batchsize'])\n",
    "    return test_data, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 14:15:41.783 | INFO     | bert.tokenization:from_pretrained:171 - loading vocabulary file data/bert/scibert_scivocab_cased/vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "\tWords found in train: 10876\n",
      "\tWords found in pre-trained only: 0\n",
      "\tWords not found anywhere: 2083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 14:15:42.477 | INFO     | bert.tokenization:from_pretrained:171 - loading vocabulary file data/bert/scibert_scivocab_cased/vocab.txt\n",
      "2022-11-18 14:15:52.648 | INFO     | bert.modeling:from_pretrained:577 - loading archive file data/bert/scibert_scivocab_cased\n",
      "2022-11-18 14:15:52.650 | INFO     | bert.modeling:from_pretrained:595 - Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 31116\n",
      "}\n",
      "\n",
      "2022-11-18 14:15:54.355 | INFO     | bert.modeling:from_pretrained:645 - Weights of NestedNERModel not initialized from pretrained model: ['label_ids', 'entity_classifier.weight', 'entity_classifier.bias', 'trigger_classifier.weight', 'trigger_classifier.bias']\n",
      "2022-11-18 14:15:54.356 | INFO     | bert.modeling:from_pretrained:648 - Weights from pretrained model not used in NestedNERModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/models/cg/model/20190911030703702499_deepee_base_92_59.49.pt\n",
      "Loading model from checkpoint data/models/cg/model/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeepEM(\n",
       "  (NER_layer): NestedNERModel(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(31116, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (entity_classifier): Linear(in_features=2304, out_features=18, bias=True)\n",
       "    (trigger_classifier): Linear(in_features=2304, out_features=40, bias=True)\n",
       "  )\n",
       "  (REL_layer): RELModel(\n",
       "    (type_embed): Embedding(59, 300, padding_idx=58)\n",
       "    (hidden_layer1): Linear(in_features=5976, out_features=1000, bias=False)\n",
       "    (hidden_layer2): Linear(in_features=1000, out_features=500, bias=False)\n",
       "    (l_class): Linear(in_features=500, out_features=19, bias=True)\n",
       "  )\n",
       "  (EV_layer): EVModel(\n",
       "    (ev_struct_generator): EV_Generator()\n",
       "    (rtype_layer): Embedding(20, 150)\n",
       "    (in_arg_layer): Linear(in_features=3254, out_features=1000, bias=False)\n",
       "    (out_arg_layer): Linear(in_features=3254, out_features=1000, bias=False)\n",
       "    (hidden_layer1): Linear(in_features=3604, out_features=1000, bias=True)\n",
       "    (hidden_layer2): Linear(in_features=1000, out_features=500, bias=True)\n",
       "    (l_class): Linear(in_features=500, out_features=1, bias=True)\n",
       "    (ev2ent_reduce): Linear(in_features=500, out_features=2604, bias=True)\n",
       "    (modality_layer): Linear(in_features=500, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inp_args = utils._parsing_jupyter()\n",
    "config_path = '/home/julio/repos/event_finder/DeepEventMine_fork/experiments/pubmed100/configs/predict-pubmed-100.yaml'\n",
    "\n",
    "# set config path manually\n",
    "# config_path = 'configs/debug.yaml'\n",
    "\n",
    "with open(config_path, 'r') as stream:\n",
    "    pred_params = utils._ordered_load(stream)\n",
    "\n",
    "# Fix seed for reproducibility\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(pred_params['seed'])\n",
    "random.seed(pred_params['seed'])\n",
    "np.random.seed(pred_params['seed'])\n",
    "torch.manual_seed(pred_params['seed'])\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Load pre-trained parameters\n",
    "with open(pred_params['saved_params'], \"rb\") as f:\n",
    "    parameters = pickle.load(f)\n",
    "\n",
    "parameters['predict'] = True\n",
    "\n",
    "# Set predict settings value for params\n",
    "parameters['gpu'] = pred_params['gpu']\n",
    "parameters['batchsize'] = pred_params['batchsize']\n",
    "print('GPU available:', torch.cuda.is_available())\n",
    "if parameters['gpu'] >= 0:\n",
    "    device = torch.device(\n",
    "        \"cuda:\" + str(parameters['gpu']) if torch.cuda.is_available() else \"cpu\")\n",
    "    # torch.cuda.set_device(parameters['gpu'])\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "parameters['device'] = device\n",
    "\n",
    "# Set evaluation settings\n",
    "parameters['test_data'] = pred_params['test_data']\n",
    "\n",
    "parameters['bert_model'] = pred_params['bert_model']\n",
    "\n",
    "result_dir = pred_params['result_dir']\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "parameters['result_dir'] = pred_params['result_dir']\n",
    "\n",
    "# raw text\n",
    "parameters['raw_text'] = pred_params['raw_text']\n",
    "parameters['ner_predict_all'] = pred_params['raw_text']\n",
    "parameters['a2_entities'] = pred_params['a2_entities']\n",
    "parameters['json_file'] = pred_params['json_file']\n",
    "\n",
    "# process data\n",
    "test_data = prepdata.prep_input_data(\n",
    "    pred_params['test_data'], parameters, json_file=parameters['json_file'])\n",
    "nntest_data, test_dataloader = read_test_data(test_data, parameters)\n",
    "\n",
    "# model\n",
    "deepee_model = deepEM.DeepEM(parameters)\n",
    "\n",
    "model_path = pred_params['model_path']\n",
    "\n",
    "# Load all models\n",
    "utils.handle_checkpoints(model=deepee_model,\n",
    "                            checkpoint_dir=model_path,\n",
    "                            params={\n",
    "                                'device': device\n",
    "                            },\n",
    "                            resume=True)\n",
    "\n",
    "deepee_model.to(device)\n",
    "\n",
    "# with profile(activities=[\n",
    "#         ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True,with_stack=True) as prof:\n",
    "#     with record_function(\"model_inference\"):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "NER LOOP: --- 4.03138279914856 seconds ---\n",
      "NER LAYER: --- 4.112555980682373 seconds ---\n",
      "REL LAYER: --- 0.015381574630737305 seconds ---\n",
      "EV LAYER: --- 0.0448606014251709 seconds ---\n",
      "ALL FOWARD LAYER: --- 4.338080883026123 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  20%|██        | 1/5 [00:06<00:27,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 6.993008375167847 seconds ---\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "NER LOOP: --- 6.95419979095459 seconds ---\n",
      "NER LAYER: --- 7.063451290130615 seconds ---\n",
      "REL LAYER: --- 0.02078723907470703 seconds ---\n",
      "EV LAYER: --- 0.13923025131225586 seconds ---\n",
      "ALL FOWARD LAYER: --- 7.529953479766846 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** KeyboardInterrupt exception caught in code being profiled."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 15.6906 s\n",
      "File: /home/julio/repos/event_finder/DeepEventMine_fork/eval/evaluate.py\n",
      "Function: predict at line 9\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     9                                           def predict(model, result_dir, eval_dataloader, eval_data, g_entity_ids_, params):\n",
      "    10         1       7129.0   7129.0      0.0      mapping_id_tag = params['mappings']['nn_mapping']['id_tag_mapping']\n",
      "    11                                           \n",
      "    12                                               # store predicted entities\n",
      "    13         1        720.0    720.0      0.0      ent_preds = []\n",
      "    14                                           \n",
      "    15                                               # store predicted events\n",
      "    16         1        411.0    411.0      0.0      ev_preds = []\n",
      "    17                                           \n",
      "    18         1       1051.0   1051.0      0.0      fidss, wordss, offsetss, sub_to_wordss, span_indicess = [], [], [], [], []\n",
      "    19                                           \n",
      "    20                                               # entity and relation output\n",
      "    21         1        471.0    471.0      0.0      ent_anns = []\n",
      "    22         1        369.0    369.0      0.0      rel_anns = []\n",
      "    23                                           \n",
      "    24                                               # Evaluation phase\n",
      "    25         1    3924850.0 3924850.0      0.0      model.eval()\n",
      "    26                                           \n",
      "    27         1        980.0    980.0      0.0      all_ner_preds, all_ner_golds, all_ner_terms = [], [], []\n",
      "    28                                           \n",
      "    29         1        371.0    371.0      0.0      is_eval_ev = False\n",
      "    30         1       1591.0   1591.0      0.0      start_time = time.time()\n",
      "    31         2    4140058.0 2070029.0      0.0      for step, batch in enumerate(\n",
      "    32         1    1353110.0 1353110.0      0.0              tqdm(eval_dataloader, desc=\"Iteration\", leave=False)\n",
      "    33                                               ):\n",
      "    34         2       3660.0   1830.0      0.0          eval_data_ids = batch\n",
      "    35         2 1511234657.0 755617328.5      9.6          tensors = utils.get_tensors(eval_data_ids, eval_data, params)\n",
      "    36                                           \n",
      "    37         2      33750.0  16875.0      0.0          nn_tokens, nn_ids, nn_token_mask, nn_attention_mask, nn_span_indices, nn_span_labels, nn_span_labels_match_rel, nn_entity_masks, nn_trigger_masks, _, \\\n",
      "    38         2       4200.0   2100.0      0.0          etypes, _ = tensors\n",
      "    39                                           \n",
      "    40         2     129109.0  64554.5      0.0          fids = [\n",
      "    41         2      16270.0   8135.0      0.0              eval_data[\"fids\"][data_id] for data_id in eval_data_ids[0].tolist()\n",
      "    42                                                   ]\n",
      "    43         2     238168.0 119084.0      0.0          offsets = [\n",
      "    44                                                       eval_data[\"offsets\"][data_id]\n",
      "    45         2       2800.0   1400.0      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    46                                                   ]\n",
      "    47         2      79350.0  39675.0      0.0          words = [\n",
      "    48         2       2620.0   1310.0      0.0              eval_data[\"words\"][data_id] for data_id in eval_data_ids[0].tolist()\n",
      "    49                                                   ]\n",
      "    50         2      88208.0  44104.0      0.0          sub_to_words = [\n",
      "    51                                                       eval_data[\"sub_to_words\"][data_id]\n",
      "    52         2       2990.0   1495.0      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    53                                                   ]\n",
      "    54         2     106869.0  53434.5      0.0          subwords = [\n",
      "    55                                                       eval_data[\"subwords\"][data_id]\n",
      "    56         2       2599.0   1299.5      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    57                                                   ]\n",
      "    58         2      61840.0  30920.0      0.0          gold_entities = [\n",
      "    59                                                       eval_data[\"entities\"][data_id]\n",
      "    60         2       2620.0   1310.0      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    61                                                   ]\n",
      "    62                                           \n",
      "    63         2      74450.0  37225.0      0.0          with torch.no_grad():\n",
      "    64         2 11868338176.0 5934169088.0     75.6              ner_out, rel_out, ev_out = model(tensors, params)\n",
      "    65                                           \n",
      "    66         2       1871.0    935.5      0.0          ner_preds = ner_out['preds']\n",
      "    67                                           \n",
      "    68         2        550.0    275.0      0.0          ner_terms = ner_out['terms']\n",
      "    69                                           \n",
      "    70         2       1480.0    740.0      0.0          all_ner_terms.append(ner_terms)\n",
      "    71                                           \n",
      "    72       139      61880.0    445.2      0.0          for sentence_idx, ner_pred in enumerate(ner_preds):\n",
      "    73       139      62624.0    450.5      0.0              all_ner_golds.append(\n",
      "    74       139     170108.0   1223.8      0.0                  [\n",
      "    75                                                               (\n",
      "    76                                                                   sub_to_words[sentence_idx][span_start],\n",
      "    77                                                                   sub_to_words[sentence_idx][span_end],\n",
      "    78                                                                   mapping_id_tag[label_id],\n",
      "    79                                                               )\n",
      "    80                                                               for (\n",
      "    81                                                                       span_start,\n",
      "    82                                                                       span_end,\n",
      "    83       139      85920.0    618.1      0.0                          ), label_ids in gold_entities[sentence_idx].items()\n",
      "    84                                                               for label_id in label_ids\n",
      "    85                                                           ]\n",
      "    86                                                       )\n",
      "    87                                           \n",
      "    88       139      35940.0    258.6      0.0              pred_entities = []\n",
      "    89     68230   30432018.0    446.0      0.2              for span_id, ner_pred_id in enumerate(ner_pred):\n",
      "    90     68230  458028103.0   6713.0      2.9                  span_start, span_end = nn_span_indices[sentence_idx][span_id]\n",
      "    91     68229 1766861511.0  25896.0     11.3                  span_start, span_end = span_start.item(), span_end.item()\n",
      "    92     67933   40322396.0    593.6      0.3                  if (ner_pred_id > 0\n",
      "    93       296     250074.0    844.8      0.0                          and span_start in sub_to_words[sentence_idx]\n",
      "    94       296      80327.0    271.4      0.0                          and span_end in sub_to_words[sentence_idx]\n",
      "    95                                                           ):\n",
      "    96       296     116054.0    392.1      0.0                      pred_entities.append(\n",
      "    97       296      96493.0    326.0      0.0                          (\n",
      "    98       296     114451.0    386.7      0.0                              sub_to_words[sentence_idx][span_start],\n",
      "    99       296      92261.0    311.7      0.0                              sub_to_words[sentence_idx][span_end],\n",
      "   100       296     197313.0    666.6      0.0                              mapping_id_tag[ner_pred_id],\n",
      "   101                                                                   )\n",
      "   102                                                               )\n",
      "   103       138      70151.0    508.3      0.0              all_ner_preds.append(pred_entities)\n",
      "   104                                           \n",
      "   105                                                   # entity prediction\n",
      "   106         1       2550.0   2550.0      0.0          ent_ann = {'span_indices': nn_span_indices, 'ner_preds': ner_out['preds'], 'words': words,\n",
      "   107         1        650.0    650.0      0.0                     'offsets': offsets, 'sub_to_words': sub_to_words, 'subwords': subwords,\n",
      "   108         1        330.0    330.0      0.0                     'ner_terms': ner_terms}\n",
      "   109         1        690.0    690.0      0.0          ent_anns.append(ent_ann)\n",
      "   110                                           \n",
      "   111         1       1070.0   1070.0      0.0          fidss.append(fids)\n",
      "   112                                           \n",
      "   113         1        710.0    710.0      0.0          wordss.append(words)\n",
      "   114         1       1850.0   1850.0      0.0          offsetss.append(offsets)\n",
      "   115         1        380.0    380.0      0.0          sub_to_wordss.append(sub_to_words)\n",
      "   116                                           \n",
      "   117                                                   # relation prediction\n",
      "   118         1        600.0    600.0      0.0          if rel_out != None:\n",
      "   119         1       1040.0   1040.0      0.0              pairs_idx = rel_out['pairs_idx']\n",
      "   120         1        460.0    460.0      0.0              rel_pred = rel_out['preds']\n",
      "   121                                           \n",
      "   122         1       5650.0   5650.0      0.0              rel_ann = {'pairs_idx': pairs_idx, 'rel_preds': rel_pred}\n",
      "   123         1        530.0    530.0      0.0              rel_anns.append(rel_ann)\n",
      "   124                                                   else:\n",
      "   125                                                       rel_anns.append({})\n",
      "   126                                           \n",
      "   127                                                   # event prediction\n",
      "   128         1        620.0    620.0      0.0          if ev_out != None:\n",
      "   129                                                       # add predicted entity\n",
      "   130         1        560.0    560.0      0.0              ent_preds.append(ner_out[\"nner_preds\"])\n",
      "   131                                           \n",
      "   132                                                       # add predicted events\n",
      "   133         1        450.0    450.0      0.0              ev_preds.append(ev_out)\n",
      "   134                                           \n",
      "   135         1        980.0    980.0      0.0              span_indicess.append(\n",
      "   136         1    3575963.0 3575963.0      0.0                  [\n",
      "   137                                                               indice.detach().cpu().numpy()\n",
      "   138         1        310.0    310.0      0.0                      for indice in ner_out[\"span_indices\"]\n",
      "   139                                                           ]\n",
      "   140                                                       )\n",
      "   141         1        620.0    620.0      0.0              is_eval_ev = True\n",
      "   142                                                   else:\n",
      "   143                                                       ent_preds.append([])\n",
      "   144                                                       ev_preds.append([])\n",
      "   145                                           \n",
      "   146                                                       span_indicess.append([])\n",
      "   147                                           \n",
      "   148                                                   # Clear GPU unused RAM:\n",
      "   149                                                   # if params['gpu'] >= 0:\n",
      "   150                                                   #     torch.cuda.empty_cache()\n",
      "   151                                           \n",
      "   152         1     101139.0 101139.0      0.0          print(\"PREDICT LOOP: --- %s seconds ---\" % (time.time() - start_time))\n",
      "   153                                           \n",
      "   154                                               file_time = time.time()        \n",
      "   155                                               # write entity and relation prediction\n",
      "   156                                               _ = write_entity_relations(\n",
      "   157                                                   result_dir=result_dir,\n",
      "   158                                                   fidss=fidss,\n",
      "   159                                                   ent_anns=ent_anns,\n",
      "   160                                                   rel_anns=rel_anns,\n",
      "   161                                                   params=params\n",
      "   162                                               )\n",
      "   163                                           \n",
      "   164                                               if is_eval_ev > 0:\n",
      "   165                                                   write_events(fids=fidss,\n",
      "   166                                                                all_ent_preds=ent_preds,\n",
      "   167                                                                all_words=wordss,\n",
      "   168                                                                all_offsets=offsetss,\n",
      "   169                                                                all_span_terms=all_ner_terms,\n",
      "   170                                                                all_span_indices=span_indicess,\n",
      "   171                                                                all_sub_to_words=sub_to_wordss,\n",
      "   172                                                                all_ev_preds=ev_preds,\n",
      "   173                                                                g_entity_ids_=g_entity_ids_,\n",
      "   174                                                                params=params,\n",
      "   175                                                                result_dir=result_dir)\n",
      "   176                                           \n",
      "   177                                               print(\"(FILE writing): --- %s seconds ---\" % (time.time() - file_time))"
     ]
    }
   ],
   "source": [
    "%lprun -f predict predict(model=deepee_model,result_dir=result_dir, eval_dataloader=test_dataloader,eval_data=nntest_data,g_entity_ids_=test_data['g_entity_ids_'],params=parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_ids[0] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]\n",
      "NER LOOP: --- 4.019585609436035 seconds ---\n",
      "NER LAYER: --- 4.100007772445679 seconds ---\n",
      "REL LAYER: --- 0.015290260314941406 seconds ---\n",
      "EV LAYER: --- 0.044412851333618164 seconds ---\n",
      "ALL FOWARD LAYER: --- 4.33444881439209 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  20%|██        | 1/5 [00:06<00:27,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 6.975859642028809 seconds ---\n",
      "data_ids[0] [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]\n",
      "NER LOOP: --- 6.958784580230713 seconds ---\n",
      "NER LAYER: --- 7.066285610198975 seconds ---\n",
      "REL LAYER: --- 0.02063274383544922 seconds ---\n",
      "EV LAYER: --- 0.13971471786499023 seconds ---\n",
      "ALL FOWARD LAYER: --- 7.517888069152832 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  40%|████      | 2/5 [00:18<00:29,  9.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 18.6567223072052 seconds ---\n",
      "data_ids[0] [256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383]\n",
      "NER LOOP: --- 6.491260528564453 seconds ---\n",
      "NER LAYER: --- 6.588133096694946 seconds ---\n",
      "REL LAYER: --- 0.018155574798583984 seconds ---\n",
      "EV LAYER: --- 0.116729736328125 seconds ---\n",
      "ALL FOWARD LAYER: --- 7.005605697631836 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  60%|██████    | 3/5 [00:29<00:20, 10.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 29.65032649040222 seconds ---\n",
      "data_ids[0] [384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511]\n",
      "NER LOOP: --- 5.789400577545166 seconds ---\n",
      "NER LAYER: --- 5.879717111587524 seconds ---\n",
      "REL LAYER: --- 0.017420291900634766 seconds ---\n",
      "EV LAYER: --- 0.0989067554473877 seconds ---\n",
      "ALL FOWARD LAYER: --- 6.28899884223938 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  80%|████████  | 4/5 [00:39<00:10, 10.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 39.50711679458618 seconds ---\n",
      "data_ids[0] [512, 513, 514, 515]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER LOOP: --- 0.23136258125305176 seconds ---\n",
      "NER LAYER: --- 0.24485397338867188 seconds ---\n",
      "REL LAYER: --- 0.0026149749755859375 seconds ---\n",
      "EV LAYER: --- 0.006009101867675781 seconds ---\n",
      "ALL FOWARD LAYER: --- 0.26873016357421875 seconds ---\n",
      "PREDICT LOOP: --- 39.9071102142334 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(FILE writing): --- 7.7569825649261475 seconds ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 47.2513 s\n",
      "File: /home/julio/repos/event_finder/DeepEventMine_fork/eval/evaluate.py\n",
      "Function: predict at line 9\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     9                                           def predict(model, result_dir, eval_dataloader, eval_data, g_entity_ids_, params):\n",
      "    10         1       3971.0   3971.0      0.0      mapping_id_tag = params['mappings']['nn_mapping']['id_tag_mapping']\n",
      "    11                                           \n",
      "    12                                               # store predicted entities\n",
      "    13         1        540.0    540.0      0.0      ent_preds = []\n",
      "    14                                           \n",
      "    15                                               # store predicted events\n",
      "    16         1        409.0    409.0      0.0      ev_preds = []\n",
      "    17                                           \n",
      "    18         1       1071.0   1071.0      0.0      fidss, wordss, offsetss, sub_to_wordss, span_indicess = [], [], [], [], []\n",
      "    19                                           \n",
      "    20                                               # entity and relation output\n",
      "    21         1        449.0    449.0      0.0      ent_anns = []\n",
      "    22         1        340.0    340.0      0.0      rel_anns = []\n",
      "    23                                           \n",
      "    24                                               # Evaluation phase\n",
      "    25         1    3478263.0 3478263.0      0.0      model.eval()\n",
      "    26                                           \n",
      "    27         1        880.0    880.0      0.0      all_ner_preds, all_ner_golds, all_ner_terms = [], [], []\n",
      "    28                                           \n",
      "    29         1        380.0    380.0      0.0      is_eval_ev = False\n",
      "    30         1       1660.0   1660.0      0.0      start_time = time.time()\n",
      "    31         5    9927531.0 1985506.2      0.0      for step, batch in enumerate(\n",
      "    32         1    1582237.0 1582237.0      0.0              tqdm(eval_dataloader, desc=\"Iteration\", leave=False)\n",
      "    33                                               ):\n",
      "    34         5       8430.0   1686.0      0.0          eval_data_ids = batch\n",
      "    35         5 2961847002.0 592369400.4      6.3          tensors = utils.get_tensors(eval_data_ids, eval_data, params)\n",
      "    36                                           \n",
      "    37         5     113190.0  22638.0      0.0          nn_tokens, nn_ids, nn_token_mask, nn_attention_mask, nn_span_indices, nn_span_labels, nn_span_labels_match_rel, nn_entity_masks, nn_trigger_masks, _, \\\n",
      "    38         5      16418.0   3283.6      0.0          etypes, _ = tensors\n",
      "    39                                           \n",
      "    40         5     252829.0  50565.8      0.0          fids = [\n",
      "    41         5      75839.0  15167.8      0.0              eval_data[\"fids\"][data_id] for data_id in eval_data_ids[0].tolist()\n",
      "    42                                                   ]\n",
      "    43         5     212987.0  42597.4      0.0          offsets = [\n",
      "    44                                                       eval_data[\"offsets\"][data_id]\n",
      "    45         5       8688.0   1737.6      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    46                                                   ]\n",
      "    47         5     141501.0  28300.2      0.0          words = [\n",
      "    48         5       7729.0   1545.8      0.0              eval_data[\"words\"][data_id] for data_id in eval_data_ids[0].tolist()\n",
      "    49                                                   ]\n",
      "    50         5     173789.0  34757.8      0.0          sub_to_words = [\n",
      "    51                                                       eval_data[\"sub_to_words\"][data_id]\n",
      "    52         5       7651.0   1530.2      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    53                                                   ]\n",
      "    54         5     213309.0  42661.8      0.0          subwords = [\n",
      "    55                                                       eval_data[\"subwords\"][data_id]\n",
      "    56         5       7560.0   1512.0      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    57                                                   ]\n",
      "    58         5     137500.0  27500.0      0.0          gold_entities = [\n",
      "    59                                                       eval_data[\"entities\"][data_id]\n",
      "    60         5       7660.0   1532.0      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    61                                                   ]\n",
      "    62                                           \n",
      "    63         5     139097.0  27819.4      0.0          with torch.no_grad():\n",
      "    64         5 25416182340.0 5083236468.0     53.8              ner_out, rel_out, ev_out = model(tensors, params)\n",
      "    65                                           \n",
      "    66         5       3259.0    651.8      0.0          ner_preds = ner_out['preds']\n",
      "    67                                           \n",
      "    68         5       1550.0    310.0      0.0          ner_terms = ner_out['terms']\n",
      "    69                                           \n",
      "    70         5       4160.0    832.0      0.0          all_ner_terms.append(ner_terms)\n",
      "    71                                           \n",
      "    72       516     224552.0    435.2      0.0          for sentence_idx, ner_pred in enumerate(ner_preds):\n",
      "    73       516     259114.0    502.2      0.0              all_ner_golds.append(\n",
      "    74       516     605782.0   1174.0      0.0                  [\n",
      "    75                                                               (\n",
      "    76                                                                   sub_to_words[sentence_idx][span_start],\n",
      "    77                                                                   sub_to_words[sentence_idx][span_end],\n",
      "    78                                                                   mapping_id_tag[label_id],\n",
      "    79                                                               )\n",
      "    80                                                               for (\n",
      "    81                                                                       span_start,\n",
      "    82                                                                       span_end,\n",
      "    83       516     307429.0    595.8      0.0                          ), label_ids in gold_entities[sentence_idx].items()\n",
      "    84                                                               for label_id in label_ids\n",
      "    85                                                           ]\n",
      "    86                                                       )\n",
      "    87                                           \n",
      "    88       516     131566.0    255.0      0.0              pred_entities = []\n",
      "    89    331296  147697022.0    445.8      0.3              for span_id, ner_pred_id in enumerate(ner_pred):\n",
      "    90    331296 2207749373.0   6664.0      4.7                  span_start, span_end = nn_span_indices[sentence_idx][span_id]\n",
      "    91    331296 8542031079.0  25783.7     18.1                  span_start, span_end = span_start.item(), span_end.item()\n",
      "    92    329435  180497540.0    547.9      0.4                  if (ner_pred_id > 0\n",
      "    93      1861    1378825.0    740.9      0.0                          and span_start in sub_to_words[sentence_idx]\n",
      "    94      1861     536946.0    288.5      0.0                          and span_end in sub_to_words[sentence_idx]\n",
      "    95                                                           ):\n",
      "    96      1861     710918.0    382.0      0.0                      pred_entities.append(\n",
      "    97      1861     508231.0    273.1      0.0                          (\n",
      "    98      1861     703319.0    377.9      0.0                              sub_to_words[sentence_idx][span_start],\n",
      "    99      1861     558925.0    300.3      0.0                              sub_to_words[sentence_idx][span_end],\n",
      "   100      1861    1180220.0    634.2      0.0                              mapping_id_tag[ner_pred_id],\n",
      "   101                                                                   )\n",
      "   102                                                               )\n",
      "   103       516     258814.0    501.6      0.0              all_ner_preds.append(pred_entities)\n",
      "   104                                           \n",
      "   105                                                   # entity prediction\n",
      "   106         5      12550.0   2510.0      0.0          ent_ann = {'span_indices': nn_span_indices, 'ner_preds': ner_out['preds'], 'words': words,\n",
      "   107         5       2301.0    460.2      0.0                     'offsets': offsets, 'sub_to_words': sub_to_words, 'subwords': subwords,\n",
      "   108         5       1380.0    276.0      0.0                     'ner_terms': ner_terms}\n",
      "   109         5       3450.0    690.0      0.0          ent_anns.append(ent_ann)\n",
      "   110                                           \n",
      "   111         5       3120.0    624.0      0.0          fidss.append(fids)\n",
      "   112                                           \n",
      "   113         5       3200.0    640.0      0.0          wordss.append(words)\n",
      "   114         5       2780.0    556.0      0.0          offsetss.append(offsets)\n",
      "   115         5       2470.0    494.0      0.0          sub_to_wordss.append(sub_to_words)\n",
      "   116                                           \n",
      "   117                                                   # relation prediction\n",
      "   118         5       2750.0    550.0      0.0          if rel_out != None:\n",
      "   119         5       3740.0    748.0      0.0              pairs_idx = rel_out['pairs_idx']\n",
      "   120         5       1740.0    348.0      0.0              rel_pred = rel_out['preds']\n",
      "   121                                           \n",
      "   122         5       5810.0   1162.0      0.0              rel_ann = {'pairs_idx': pairs_idx, 'rel_preds': rel_pred}\n",
      "   123         5       2411.0    482.2      0.0              rel_anns.append(rel_ann)\n",
      "   124                                                   else:\n",
      "   125                                                       rel_anns.append({})\n",
      "   126                                           \n",
      "   127                                                   # event prediction\n",
      "   128         5       1989.0    397.8      0.0          if ev_out != None:\n",
      "   129                                                       # add predicted entity\n",
      "   130         5       3029.0    605.8      0.0              ent_preds.append(ner_out[\"nner_preds\"])\n",
      "   131                                           \n",
      "   132                                                       # add predicted events\n",
      "   133         5       3090.0    618.0      0.0              ev_preds.append(ev_out)\n",
      "   134                                           \n",
      "   135         5       3310.0    662.0      0.0              span_indicess.append(\n",
      "   136         5   13964646.0 2792929.2      0.0                  [\n",
      "   137                                                               indice.detach().cpu().numpy()\n",
      "   138         5       1519.0    303.8      0.0                      for indice in ner_out[\"span_indices\"]\n",
      "   139                                                           ]\n",
      "   140                                                       )\n",
      "   141         5       2682.0    536.4      0.0              is_eval_ev = True\n",
      "   142                                                   else:\n",
      "   143                                                       ent_preds.append([])\n",
      "   144                                                       ev_preds.append([])\n",
      "   145                                           \n",
      "   146                                                       span_indicess.append([])\n",
      "   147                                           \n",
      "   148                                                   # Clear GPU unused RAM:\n",
      "   149                                                   # if params['gpu'] >= 0:\n",
      "   150                                                   #     torch.cuda.empty_cache()\n",
      "   151                                           \n",
      "   152         5     313887.0  62777.4      0.0          print(\"PREDICT LOOP: --- %s seconds ---\" % (time.time() - start_time))\n",
      "   153                                           \n",
      "   154         1        731.0    731.0      0.0      file_time = time.time()        \n",
      "   155                                               # write entity and relation prediction\n",
      "   156         1 6716336481.0 6716336481.0     14.2      _ = write_entity_relations(\n",
      "   157         1        280.0    280.0      0.0          result_dir=result_dir,\n",
      "   158         1        269.0    269.0      0.0          fidss=fidss,\n",
      "   159         1        240.0    240.0      0.0          ent_anns=ent_anns,\n",
      "   160         1        220.0    220.0      0.0          rel_anns=rel_anns,\n",
      "   161         1        250.0    250.0      0.0          params=params\n",
      "   162                                               )\n",
      "   163                                           \n",
      "   164         1       1110.0   1110.0      0.0      if is_eval_ev > 0:\n",
      "   165         1 1040583273.0 1040583273.0      2.2          write_events(fids=fidss,\n",
      "   166         1        400.0    400.0      0.0                       all_ent_preds=ent_preds,\n",
      "   167         1        490.0    490.0      0.0                       all_words=wordss,\n",
      "   168         1        490.0    490.0      0.0                       all_offsets=offsetss,\n",
      "   169         1        250.0    250.0      0.0                       all_span_terms=all_ner_terms,\n",
      "   170         1        450.0    450.0      0.0                       all_span_indices=span_indicess,\n",
      "   171         1        750.0    750.0      0.0                       all_sub_to_words=sub_to_wordss,\n",
      "   172         1        650.0    650.0      0.0                       all_ev_preds=ev_preds,\n",
      "   173         1        360.0    360.0      0.0                       g_entity_ids_=g_entity_ids_,\n",
      "   174         1        230.0    230.0      0.0                       params=params,\n",
      "   175         1        250.0    250.0      0.0                       result_dir=result_dir)\n",
      "   176                                           \n",
      "   177         1     133199.0 133199.0      0.0      print(\"(FILE writing): --- %s seconds ---\" % (time.time() - file_time))"
     ]
    }
   ],
   "source": [
    "%lprun -f predict predict(model=deepee_model, result_dir=result_dir, eval_dataloader=test_dataloader, eval_data=nntest_data, g_entity_ids_=test_data['g_entity_ids_'], params=parameters)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('ontonerd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c215c50f3ee9fb94f7abb07f40a8d094769bf1b1c1172e473819ed198c6051f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
