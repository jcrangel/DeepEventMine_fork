{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 09:01:45.478 | INFO     | bert.modeling:<module>:231 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "from eval.evaluate import predict\n",
    "\n",
    "from nets import deepEM\n",
    "from loader.prepData import prepdata\n",
    "from loader.prepNN import prep4nn\n",
    "from utils import utils\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_test_data(test_data, params):\n",
    "    test = prep4nn.data2network(test_data, 'predict', params)\n",
    "\n",
    "    if len(test) == 0:\n",
    "        raise ValueError(\"Test set empty.\")\n",
    "\n",
    "    test_data = prep4nn.torch_data_2_network(\n",
    "        cdata2network=test, params=params, do_get_nn_data=True)\n",
    "    te_data_size = len(test_data['nn_data']['ids'])\n",
    "\n",
    "    test_data_ids = TensorDataset(torch.arange(te_data_size))\n",
    "    test_sampler = SequentialSampler(test_data_ids)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data_ids, sampler=test_sampler, batch_size=params['batchsize'])\n",
    "    return test_data, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 09:02:38.297 | INFO     | bert.tokenization:from_pretrained:171 - loading vocabulary file data/bert/scibert_scivocab_cased/vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "\tWords found in train: 10876\n",
      "\tWords found in pre-trained only: 0\n",
      "\tWords not found anywhere: 2083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 09:02:38.986 | INFO     | bert.tokenization:from_pretrained:171 - loading vocabulary file data/bert/scibert_scivocab_cased/vocab.txt\n",
      "2022-11-21 09:02:49.155 | INFO     | bert.modeling:from_pretrained:577 - loading archive file data/bert/scibert_scivocab_cased\n",
      "2022-11-21 09:02:49.158 | INFO     | bert.modeling:from_pretrained:595 - Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 31116\n",
      "}\n",
      "\n",
      "2022-11-21 09:02:51.003 | INFO     | bert.modeling:from_pretrained:645 - Weights of NestedNERModel not initialized from pretrained model: ['label_ids', 'entity_classifier.weight', 'entity_classifier.bias', 'trigger_classifier.weight', 'trigger_classifier.bias']\n",
      "2022-11-21 09:02:51.004 | INFO     | bert.modeling:from_pretrained:648 - Weights from pretrained model not used in NestedNERModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/models/cg/model/20190911030703702499_deepee_base_92_59.49.pt\n",
      "Loading model from checkpoint data/models/cg/model/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeepEM(\n",
       "  (NER_layer): NestedNERModel(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(31116, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (entity_classifier): Linear(in_features=2304, out_features=18, bias=True)\n",
       "    (trigger_classifier): Linear(in_features=2304, out_features=40, bias=True)\n",
       "  )\n",
       "  (REL_layer): RELModel(\n",
       "    (type_embed): Embedding(59, 300, padding_idx=58)\n",
       "    (hidden_layer1): Linear(in_features=5976, out_features=1000, bias=False)\n",
       "    (hidden_layer2): Linear(in_features=1000, out_features=500, bias=False)\n",
       "    (l_class): Linear(in_features=500, out_features=19, bias=True)\n",
       "  )\n",
       "  (EV_layer): EVModel(\n",
       "    (ev_struct_generator): EV_Generator()\n",
       "    (rtype_layer): Embedding(20, 150)\n",
       "    (in_arg_layer): Linear(in_features=3254, out_features=1000, bias=False)\n",
       "    (out_arg_layer): Linear(in_features=3254, out_features=1000, bias=False)\n",
       "    (hidden_layer1): Linear(in_features=3604, out_features=1000, bias=True)\n",
       "    (hidden_layer2): Linear(in_features=1000, out_features=500, bias=True)\n",
       "    (l_class): Linear(in_features=500, out_features=1, bias=True)\n",
       "    (ev2ent_reduce): Linear(in_features=500, out_features=2604, bias=True)\n",
       "    (modality_layer): Linear(in_features=500, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inp_args = utils._parsing_jupyter()\n",
    "config_path = '/home/julio/repos/event_finder/DeepEventMine_fork/experiments/pubmed100/configs/predict-pubmed-100.yaml'\n",
    "\n",
    "# set config path manually\n",
    "# config_path = 'configs/debug.yaml'\n",
    "\n",
    "with open(config_path, 'r') as stream:\n",
    "    pred_params = utils._ordered_load(stream)\n",
    "\n",
    "# Fix seed for reproducibility\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(pred_params['seed'])\n",
    "random.seed(pred_params['seed'])\n",
    "np.random.seed(pred_params['seed'])\n",
    "torch.manual_seed(pred_params['seed'])\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Load pre-trained parameters\n",
    "with open(pred_params['saved_params'], \"rb\") as f:\n",
    "    parameters = pickle.load(f)\n",
    "\n",
    "parameters['predict'] = True\n",
    "\n",
    "# Set predict settings value for params\n",
    "parameters['gpu'] = pred_params['gpu']\n",
    "parameters['batchsize'] = pred_params['batchsize']\n",
    "print('GPU available:', torch.cuda.is_available())\n",
    "if parameters['gpu'] >= 0:\n",
    "    device = torch.device(\n",
    "        \"cuda:\" + str(parameters['gpu']) if torch.cuda.is_available() else \"cpu\")\n",
    "    # torch.cuda.set_device(parameters['gpu'])\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "parameters['device'] = device\n",
    "\n",
    "# Set evaluation settings\n",
    "parameters['test_data'] = pred_params['test_data']\n",
    "\n",
    "parameters['bert_model'] = pred_params['bert_model']\n",
    "\n",
    "result_dir = pred_params['result_dir']\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "parameters['result_dir'] = pred_params['result_dir']\n",
    "\n",
    "# raw text\n",
    "parameters['raw_text'] = pred_params['raw_text']\n",
    "parameters['ner_predict_all'] = pred_params['raw_text']\n",
    "parameters['a2_entities'] = pred_params['a2_entities']\n",
    "parameters['json_file'] = pred_params['json_file']\n",
    "\n",
    "# process data\n",
    "test_data = prepdata.prep_input_data(\n",
    "    pred_params['test_data'], parameters, json_file=parameters['json_file'])\n",
    "nntest_data, test_dataloader = read_test_data(test_data, parameters)\n",
    "\n",
    "# model\n",
    "deepee_model = deepEM.DeepEM(parameters)\n",
    "\n",
    "model_path = pred_params['model_path']\n",
    "\n",
    "# Load all models\n",
    "utils.handle_checkpoints(model=deepee_model,\n",
    "                            checkpoint_dir=model_path,\n",
    "                            params={\n",
    "                                'device': device\n",
    "                            },\n",
    "                            resume=True)\n",
    "\n",
    "deepee_model.to(device)\n",
    "\n",
    "# with profile(activities=[\n",
    "#         ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True,with_stack=True) as prof:\n",
    "#     with record_function(\"model_inference\"):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/5 [00:00<?, ?it/s]/home/julio/repos/event_finder/DeepEventMine_fork/nets/NERNet.py:59: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:30.)\n",
      "  flattened_embedding_indices = torch.arange(\n",
      "/home/julio/repos/event_finder/DeepEventMine_fork/nets/NERNet.py:93: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:30.)\n",
      "  flattened_sentence_indices = sentence_indices.flatten().masked_select(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER LOOP: --- 0.37952613830566406 seconds ---\n",
      "NER LAYER: --- 0.5120458602905273 seconds ---\n",
      "REL LAYER: --- 0.022736072540283203 seconds ---\n",
      "EV LAYER: --- 0.05231738090515137 seconds ---\n",
      "ALL FOWARD LAYER: --- 0.7793147563934326 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  20%|██        | 1/5 [00:01<00:06,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 1.7276966571807861 seconds ---\n",
      "NER LOOP: --- 0.8099792003631592 seconds ---\n",
      "NER LAYER: --- 0.9124910831451416 seconds ---\n",
      "REL LAYER: --- 0.022560834884643555 seconds ---\n",
      "EV LAYER: --- 0.13768768310546875 seconds ---\n",
      "ALL FOWARD LAYER: --- 1.3659417629241943 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  40%|████      | 2/5 [00:04<00:06,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 4.445741415023804 seconds ---\n",
      "NER LOOP: --- 0.6077940464019775 seconds ---\n",
      "NER LAYER: --- 0.6996157169342041 seconds ---\n",
      "REL LAYER: --- 0.019855737686157227 seconds ---\n",
      "EV LAYER: --- 0.11309576034545898 seconds ---\n",
      "ALL FOWARD LAYER: --- 1.1152832508087158 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  60%|██████    | 3/5 [00:06<00:04,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 6.772505521774292 seconds ---\n",
      "NER LOOP: --- 0.5288283824920654 seconds ---\n",
      "NER LAYER: --- 0.6218419075012207 seconds ---\n",
      "REL LAYER: --- 0.0190579891204834 seconds ---\n",
      "EV LAYER: --- 0.09977316856384277 seconds ---\n",
      "ALL FOWARD LAYER: --- 0.9954302310943604 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 8.90893292427063 seconds ---\n",
      "NER LOOP: --- 0.021910429000854492 seconds ---\n",
      "NER LAYER: --- 0.0431976318359375 seconds ---\n",
      "REL LAYER: --- 0.002622365951538086 seconds ---\n",
      "EV LAYER: --- 0.006018161773681641 seconds ---\n",
      "ALL FOWARD LAYER: --- 0.06672978401184082 seconds ---\n",
      "PREDICT LOOP: --- 9.01675033569336 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(FILE writing): --- 7.4954822063446045 seconds ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 16.1899 s\n",
      "File: /home/julio/repos/event_finder/DeepEventMine_fork/eval/evaluate.py\n",
      "Function: predict at line 9\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     9                                           def predict(model, result_dir, eval_dataloader, eval_data, g_entity_ids_, params):\n",
      "    10         1       4240.0   4240.0      0.0      mapping_id_tag = params['mappings']['nn_mapping']['id_tag_mapping']\n",
      "    11                                           \n",
      "    12                                               # store predicted entities\n",
      "    13         1        240.0    240.0      0.0      ent_preds = []\n",
      "    14                                           \n",
      "    15                                               # store predicted events\n",
      "    16         1        160.0    160.0      0.0      ev_preds = []\n",
      "    17                                           \n",
      "    18         1        550.0    550.0      0.0      fidss, wordss, offsetss, sub_to_wordss, span_indicess = [], [], [], [], []\n",
      "    19                                           \n",
      "    20                                               # entity and relation output\n",
      "    21         1        230.0    230.0      0.0      ent_anns = []\n",
      "    22         1        410.0    410.0      0.0      rel_anns = []\n",
      "    23                                           \n",
      "    24                                               # Evaluation phase\n",
      "    25         1    1554459.0 1554459.0      0.0      model.eval()\n",
      "    26                                           \n",
      "    27         1        569.0    569.0      0.0      all_ner_preds, all_ner_golds, all_ner_terms = [], [], []\n",
      "    28                                           \n",
      "    29         1        171.0    171.0      0.0      is_eval_ev = False\n",
      "    30         1        820.0    820.0      0.0      start_time = time.time()\n",
      "    31         5   11866733.0 2373346.6      0.1      for step, batch in enumerate(\n",
      "    32         1    1955297.0 1955297.0      0.0              tqdm(eval_dataloader, desc=\"Iteration\", leave=False)\n",
      "    33                                               ):\n",
      "    34         5       5550.0   1110.0      0.0          eval_data_ids = batch\n",
      "    35         5 3157913439.0 631582687.8     19.5          tensors = utils.get_tensors(eval_data_ids, eval_data, params)\n",
      "    36                                           \n",
      "    37         5      87720.0  17544.0      0.0          nn_tokens, nn_ids, nn_token_mask, nn_attention_mask, nn_span_indices, nn_span_labels, nn_span_labels_match_rel, nn_entity_masks, nn_trigger_masks, _, \\\n",
      "    38         5      13029.0   2605.8      0.0          etypes, _ = tensors\n",
      "    39                                           \n",
      "    40         5     259137.0  51827.4      0.0          fids = [\n",
      "    41         5      36950.0   7390.0      0.0              eval_data[\"fids\"][data_id] for data_id in eval_data_ids[0].tolist()\n",
      "    42                                                   ]\n",
      "    43         5     213110.0  42622.0      0.0          offsets = [\n",
      "    44                                                       eval_data[\"offsets\"][data_id]\n",
      "    45         5       9250.0   1850.0      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    46                                                   ]\n",
      "    47         5     158327.0  31665.4      0.0          words = [\n",
      "    48         5       7999.0   1599.8      0.0              eval_data[\"words\"][data_id] for data_id in eval_data_ids[0].tolist()\n",
      "    49                                                   ]\n",
      "    50         5     176679.0  35335.8      0.0          sub_to_words = [\n",
      "    51                                                       eval_data[\"sub_to_words\"][data_id]\n",
      "    52         5       7790.0   1558.0      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    53                                                   ]\n",
      "    54         5     227580.0  45516.0      0.0          subwords = [\n",
      "    55                                                       eval_data[\"subwords\"][data_id]\n",
      "    56         5       8000.0   1600.0      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    57                                                   ]\n",
      "    58         5     132459.0  26491.8      0.0          gold_entities = [\n",
      "    59                                                       eval_data[\"entities\"][data_id]\n",
      "    60         5       8250.0   1650.0      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    61                                                   ]\n",
      "    62                                           \n",
      "    63         5     111810.0  22362.0      0.0          with torch.no_grad():\n",
      "    64         5 4323213753.0 864642750.6     26.7              ner_out, rel_out, ev_out = model(tensors, params)\n",
      "    65                                           \n",
      "    66         5       2500.0    500.0      0.0          ner_preds = ner_out['preds']\n",
      "    67                                           \n",
      "    68         5       1520.0    304.0      0.0          ner_terms = ner_out['terms']\n",
      "    69                                           \n",
      "    70         5       3860.0    772.0      0.0          all_ner_terms.append(ner_terms)\n",
      "    71                                           \n",
      "    72       516     213744.0    414.2      0.0          for sentence_idx, ner_pred in enumerate(ner_preds):\n",
      "    73       516     172665.0    334.6      0.0              all_ner_golds.append(\n",
      "    74       516     546699.0   1059.5      0.0                  [\n",
      "    75                                                               (\n",
      "    76                                                                   sub_to_words[sentence_idx][span_start],\n",
      "    77                                                                   sub_to_words[sentence_idx][span_end],\n",
      "    78                                                                   mapping_id_tag[label_id],\n",
      "    79                                                               )\n",
      "    80                                                               for (\n",
      "    81                                                                       span_start,\n",
      "    82                                                                       span_end,\n",
      "    83       516     280460.0    543.5      0.0                          ), label_ids in gold_entities[sentence_idx].items()\n",
      "    84                                                               for label_id in label_ids\n",
      "    85                                                           ]\n",
      "    86                                                       )\n",
      "    87       516  232799266.0 451161.4      1.4              nn_span = nn_span_indices.detach().cpu().numpy()\n",
      "    88       516     184633.0    357.8      0.0              pred_entities = []  # nn_span_indice :orch.Size([128, 2450, 2]), sentence_idx:0,span_id:0\n",
      "    89    331296  109987775.0    332.0      0.7              for span_id, ner_pred_id in enumerate(ner_pred):\n",
      "    90    331296  522552214.0   1577.3      3.2                  span_start, span_end = nn_span[sentence_idx][span_id]\n",
      "    91                                                           # indx = torch.tensor([sentence_idx, span_id],\n",
      "    92                                                           #                     device=0).unsqueeze(-1)\n",
      "    93                                                           # span_start, span_end = torch.gather(nn_span_indices,1,indx)\n",
      "    94                                           \n",
      "    95                                                           # span_start, span_end = span_start.item(), span_end.item()\n",
      "    96    329435  156142044.0    474.0      1.0                  if (ner_pred_id > 0\n",
      "    97      1861    1437136.0    772.2      0.0                          and span_start in sub_to_words[sentence_idx]\n",
      "    98      1861     858725.0    461.4      0.0                          and span_end in sub_to_words[sentence_idx]\n",
      "    99                                                           ):\n",
      "   100      1861     663867.0    356.7      0.0                      pred_entities.append(\n",
      "   101      1861     542506.0    291.5      0.0                          (\n",
      "   102      1861     924745.0    496.9      0.0                              sub_to_words[sentence_idx][span_start],\n",
      "   103      1861     867406.0    466.1      0.0                              sub_to_words[sentence_idx][span_end],\n",
      "   104      1861     895785.0    481.3      0.0                              mapping_id_tag[ner_pred_id],\n",
      "   105                                                                   )\n",
      "   106                                                               )\n",
      "   107       516     242409.0    469.8      0.0              all_ner_preds.append(pred_entities)\n",
      "   108                                           \n",
      "   109                                                   # entity prediction\n",
      "   110         5       8571.0   1714.2      0.0          ent_ann = {'span_indices': nn_span_indices, 'ner_preds': ner_out['preds'], 'words': words,\n",
      "   111         5       2151.0    430.2      0.0                     'offsets': offsets, 'sub_to_words': sub_to_words, 'subwords': subwords,\n",
      "   112         5       1260.0    252.0      0.0                     'ner_terms': ner_terms}\n",
      "   113         5       2660.0    532.0      0.0          ent_anns.append(ent_ann)\n",
      "   114                                           \n",
      "   115         5       2990.0    598.0      0.0          fidss.append(fids)\n",
      "   116                                           \n",
      "   117         5       2730.0    546.0      0.0          wordss.append(words)\n",
      "   118         5       2070.0    414.0      0.0          offsetss.append(offsets)\n",
      "   119         5       2930.0    586.0      0.0          sub_to_wordss.append(sub_to_words)\n",
      "   120                                           \n",
      "   121                                                   # relation prediction\n",
      "   122         5       3160.0    632.0      0.0          if rel_out != None:\n",
      "   123         5       1770.0    354.0      0.0              pairs_idx = rel_out['pairs_idx']\n",
      "   124         5       1770.0    354.0      0.0              rel_pred = rel_out['preds']\n",
      "   125                                           \n",
      "   126         5       3560.0    712.0      0.0              rel_ann = {'pairs_idx': pairs_idx, 'rel_preds': rel_pred}\n",
      "   127         5       3040.0    608.0      0.0              rel_anns.append(rel_ann)\n",
      "   128                                                   else:\n",
      "   129                                                       rel_anns.append({})\n",
      "   130                                           \n",
      "   131                                                   # event prediction\n",
      "   132         5       2340.0    468.0      0.0          if ev_out != None:\n",
      "   133                                                       # add predicted entity\n",
      "   134         5       2290.0    458.0      0.0              ent_preds.append(ner_out[\"nner_preds\"])\n",
      "   135                                           \n",
      "   136                                                       # add predicted events\n",
      "   137         5       3540.0    708.0      0.0              ev_preds.append(ev_out)\n",
      "   138                                           \n",
      "   139         5       2881.0    576.2      0.0              span_indicess.append(\n",
      "   140         5   12806499.0 2561299.8      0.1                  [\n",
      "   141                                                               indice.detach().cpu().numpy()\n",
      "   142         5       1490.0    298.0      0.0                      for indice in ner_out[\"span_indices\"]\n",
      "   143                                                           ]\n",
      "   144                                                       )\n",
      "   145         5       1731.0    346.2      0.0              is_eval_ev = True\n",
      "   146                                                   else:\n",
      "   147                                                       ent_preds.append([])\n",
      "   148                                                       ev_preds.append([])\n",
      "   149                                           \n",
      "   150                                                       span_indicess.append([])\n",
      "   151                                           \n",
      "   152                                                   # Clear GPU unused RAM:\n",
      "   153         5       5270.0   1054.0      0.0          if params['gpu'] >= 0:\n",
      "   154         5  153645160.0 30729032.0      0.9              torch.cuda.empty_cache()\n",
      "   155                                           \n",
      "   156         5     534756.0 106951.2      0.0          print(\"PREDICT LOOP: --- %s seconds ---\" % (time.time() - start_time))\n",
      "   157                                           \n",
      "   158         1        620.0    620.0      0.0      file_time = time.time()        \n",
      "   159                                               # write entity and relation prediction\n",
      "   160         1 6657387428.0 6657387428.0     41.1      _ = write_entity_relations(\n",
      "   161         1        280.0    280.0      0.0          result_dir=result_dir,\n",
      "   162         1        260.0    260.0      0.0          fidss=fidss,\n",
      "   163         1        350.0    350.0      0.0          ent_anns=ent_anns,\n",
      "   164         1        240.0    240.0      0.0          rel_anns=rel_anns,\n",
      "   165         1        230.0    230.0      0.0          params=params\n",
      "   166                                               )\n",
      "   167                                           \n",
      "   168         1        640.0    640.0      0.0      if is_eval_ev > 0:\n",
      "   169         1  838050045.0 838050045.0      5.2          write_events(fids=fidss,\n",
      "   170         1        460.0    460.0      0.0                       all_ent_preds=ent_preds,\n",
      "   171         1        409.0    409.0      0.0                       all_words=wordss,\n",
      "   172         1        420.0    420.0      0.0                       all_offsets=offsetss,\n",
      "   173         1        380.0    380.0      0.0                       all_span_terms=all_ner_terms,\n",
      "   174         1        331.0    331.0      0.0                       all_span_indices=span_indicess,\n",
      "   175         1        551.0    551.0      0.0                       all_sub_to_words=sub_to_wordss,\n",
      "   176         1        351.0    351.0      0.0                       all_ev_preds=ev_preds,\n",
      "   177         1        351.0    351.0      0.0                       g_entity_ids_=g_entity_ids_,\n",
      "   178         1        260.0    260.0      0.0                       params=params,\n",
      "   179         1        249.0    249.0      0.0                       result_dir=result_dir)\n",
      "   180                                           \n",
      "   181         1     127709.0 127709.0      0.0      print(\"(FILE writing): --- %s seconds ---\" % (time.time() - file_time))"
     ]
    }
   ],
   "source": [
    "%lprun -f predict predict(model=deepee_model,result_dir=result_dir, eval_dataloader=test_dataloader,eval_data=nntest_data,g_entity_ids_=test_data['g_entity_ids_'],params=parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER LOOP: --- 4.088160753250122 seconds ---\n",
      "NER LAYER: --- 4.168576955795288 seconds ---\n",
      "REL LAYER: --- 0.01534891128540039 seconds ---\n",
      "EV LAYER: --- 0.04376649856567383 seconds ---\n",
      "ALL FOWARD LAYER: --- 4.417277812957764 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  20%|██        | 1/5 [00:07<00:28,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 7.059613227844238 seconds ---\n",
      "NER LOOP: --- 6.67397665977478 seconds ---\n",
      "NER LAYER: --- 6.782450199127197 seconds ---\n",
      "REL LAYER: --- 0.020621299743652344 seconds ---\n",
      "EV LAYER: --- 0.1400163173675537 seconds ---\n",
      "ALL FOWARD LAYER: --- 7.2468602657318115 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  40%|████      | 2/5 [00:18<00:28,  9.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 18.449894428253174 seconds ---\n",
      "NER LOOP: --- 6.6860737800598145 seconds ---\n",
      "NER LAYER: --- 6.782872915267944 seconds ---\n",
      "REL LAYER: --- 0.018086910247802734 seconds ---\n",
      "EV LAYER: --- 0.11701488494873047 seconds ---\n",
      "ALL FOWARD LAYER: --- 7.209285259246826 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  60%|██████    | 3/5 [00:29<00:20, 10.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 29.643948793411255 seconds ---\n",
      "NER LOOP: --- 5.759660959243774 seconds ---\n",
      "NER LAYER: --- 5.8500213623046875 seconds ---\n",
      "REL LAYER: --- 0.017380952835083008 seconds ---\n",
      "EV LAYER: --- 0.09950733184814453 seconds ---\n",
      "ALL FOWARD LAYER: --- 6.2324864864349365 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  80%|████████  | 4/5 [00:39<00:10, 10.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 39.43154978752136 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER LOOP: --- 0.2331676483154297 seconds ---\n",
      "NER LAYER: --- 0.24660420417785645 seconds ---\n",
      "REL LAYER: --- 0.01309823989868164 seconds ---\n",
      "EV LAYER: --- 0.009708166122436523 seconds ---\n",
      "ALL FOWARD LAYER: --- 0.2849557399749756 seconds ---\n",
      "PREDICT LOOP: --- 39.852705001831055 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(FILE writing): --- 7.950889587402344 seconds ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 47.3856 s\n",
      "File: /home/julio/repos/event_finder/DeepEventMine_fork/eval/evaluate.py\n",
      "Function: predict at line 9\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     9                                           def predict(model, result_dir, eval_dataloader, eval_data, g_entity_ids_, params):\n",
      "    10         1       2630.0   2630.0      0.0      mapping_id_tag = params['mappings']['nn_mapping']['id_tag_mapping']\n",
      "    11                                           \n",
      "    12                                               # store predicted entities\n",
      "    13         1        220.0    220.0      0.0      ent_preds = []\n",
      "    14                                           \n",
      "    15                                               # store predicted events\n",
      "    16         1        170.0    170.0      0.0      ev_preds = []\n",
      "    17                                           \n",
      "    18         1        430.0    430.0      0.0      fidss, wordss, offsetss, sub_to_wordss, span_indicess = [], [], [], [], []\n",
      "    19                                           \n",
      "    20                                               # entity and relation output\n",
      "    21         1        180.0    180.0      0.0      ent_anns = []\n",
      "    22         1        150.0    150.0      0.0      rel_anns = []\n",
      "    23                                           \n",
      "    24                                               # Evaluation phase\n",
      "    25         1    1584648.0 1584648.0      0.0      model.eval()\n",
      "    26                                           \n",
      "    27         1        380.0    380.0      0.0      all_ner_preds, all_ner_golds, all_ner_terms = [], [], []\n",
      "    28                                           \n",
      "    29         1        140.0    140.0      0.0      is_eval_ev = False\n",
      "    30         1        650.0    650.0      0.0      start_time = time.time()\n",
      "    31         5    8868541.0 1773708.2      0.0      for step, batch in enumerate(\n",
      "    32         1    1533128.0 1533128.0      0.0              tqdm(eval_dataloader, desc=\"Iteration\", leave=False)\n",
      "    33                                               ):\n",
      "    34         5       9869.0   1973.8      0.0          eval_data_ids = batch\n",
      "    35         5 2925576103.0 585115220.6      6.2          tensors = utils.get_tensors(eval_data_ids, eval_data, params)\n",
      "    36                                           \n",
      "    37         5      97070.0  19414.0      0.0          nn_tokens, nn_ids, nn_token_mask, nn_attention_mask, nn_span_indices, nn_span_labels, nn_span_labels_match_rel, nn_entity_masks, nn_trigger_masks, _, \\\n",
      "    38         5      15270.0   3054.0      0.0          etypes, _ = tensors\n",
      "    39                                           \n",
      "    40         5     261317.0  52263.4      0.0          fids = [\n",
      "    41         5      44899.0   8979.8      0.0              eval_data[\"fids\"][data_id] for data_id in eval_data_ids[0].tolist()\n",
      "    42                                                   ]\n",
      "    43         5     220490.0  44098.0      0.0          offsets = [\n",
      "    44                                                       eval_data[\"offsets\"][data_id]\n",
      "    45         5       8960.0   1792.0      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    46                                                   ]\n",
      "    47         5     145028.0  29005.6      0.0          words = [\n",
      "    48         5       7790.0   1558.0      0.0              eval_data[\"words\"][data_id] for data_id in eval_data_ids[0].tolist()\n",
      "    49                                                   ]\n",
      "    50         5     174989.0  34997.8      0.0          sub_to_words = [\n",
      "    51                                                       eval_data[\"sub_to_words\"][data_id]\n",
      "    52         5       7970.0   1594.0      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    53                                                   ]\n",
      "    54         5     217237.0  43447.4      0.0          subwords = [\n",
      "    55                                                       eval_data[\"subwords\"][data_id]\n",
      "    56         5       7611.0   1522.2      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    57                                                   ]\n",
      "    58         5     127589.0  25517.8      0.0          gold_entities = [\n",
      "    59                                                       eval_data[\"entities\"][data_id]\n",
      "    60         5       7621.0   1524.2      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    61                                                   ]\n",
      "    62                                           \n",
      "    63         5     113351.0  22670.2      0.0          with torch.no_grad():\n",
      "    64         5 25391459058.0 5078291811.6     53.6              ner_out, rel_out, ev_out = model(tensors, params)\n",
      "    65                                           \n",
      "    66         5       3309.0    661.8      0.0          ner_preds = ner_out['preds']\n",
      "    67                                           \n",
      "    68         5       1288.0    257.6      0.0          ner_terms = ner_out['terms']\n",
      "    69                                           \n",
      "    70         5       4980.0    996.0      0.0          all_ner_terms.append(ner_terms)\n",
      "    71                                           \n",
      "    72       516     214539.0    415.8      0.0          for sentence_idx, ner_pred in enumerate(ner_preds):\n",
      "    73       516     262156.0    508.1      0.0              all_ner_golds.append(\n",
      "    74       516     601011.0   1164.8      0.0                  [\n",
      "    75                                                               (\n",
      "    76                                                                   sub_to_words[sentence_idx][span_start],\n",
      "    77                                                                   sub_to_words[sentence_idx][span_end],\n",
      "    78                                                                   mapping_id_tag[label_id],\n",
      "    79                                                               )\n",
      "    80                                                               for (\n",
      "    81                                                                       span_start,\n",
      "    82                                                                       span_end,\n",
      "    83       516     320289.0    620.7      0.0                          ), label_ids in gold_entities[sentence_idx].items()\n",
      "    84                                                               for label_id in label_ids\n",
      "    85                                                           ]\n",
      "    86                                                       )\n",
      "    87                                           \n",
      "    88       516     138847.0    269.1      0.0              pred_entities = []\n",
      "    89    331296  144147226.0    435.1      0.3              for span_id, ner_pred_id in enumerate(ner_pred):\n",
      "    90    331296 2209194422.0   6668.3      4.7                  span_start, span_end = nn_span_indices[sentence_idx][span_id]\n",
      "    91    331296 8546913531.0  25798.4     18.0                  span_start, span_end = span_start.item(), span_end.item()\n",
      "    92    329435  182987073.0    555.5      0.4                  if (ner_pred_id > 0\n",
      "    93      1861    1239498.0    666.0      0.0                          and span_start in sub_to_words[sentence_idx]\n",
      "    94      1861     513457.0    275.9      0.0                          and span_end in sub_to_words[sentence_idx]\n",
      "    95                                                           ):\n",
      "    96      1861     721754.0    387.8      0.0                      pred_entities.append(\n",
      "    97      1861     543594.0    292.1      0.0                          (\n",
      "    98      1861     693052.0    372.4      0.0                              sub_to_words[sentence_idx][span_start],\n",
      "    99      1861     552394.0    296.8      0.0                              sub_to_words[sentence_idx][span_end],\n",
      "   100      1861    1209304.0    649.8      0.0                              mapping_id_tag[ner_pred_id],\n",
      "   101                                                                   )\n",
      "   102                                                               )\n",
      "   103       516     266167.0    515.8      0.0              all_ner_preds.append(pred_entities)\n",
      "   104                                           \n",
      "   105                                                   # entity prediction\n",
      "   106         5      14298.0   2859.6      0.0          ent_ann = {'span_indices': nn_span_indices, 'ner_preds': ner_out['preds'], 'words': words,\n",
      "   107         5       2830.0    566.0      0.0                     'offsets': offsets, 'sub_to_words': sub_to_words, 'subwords': subwords,\n",
      "   108         5       1390.0    278.0      0.0                     'ner_terms': ner_terms}\n",
      "   109         5       4241.0    848.2      0.0          ent_anns.append(ent_ann)\n",
      "   110                                           \n",
      "   111         5       3901.0    780.2      0.0          fidss.append(fids)\n",
      "   112                                           \n",
      "   113         5       4391.0    878.2      0.0          wordss.append(words)\n",
      "   114         5       3131.0    626.2      0.0          offsetss.append(offsets)\n",
      "   115         5       2589.0    517.8      0.0          sub_to_wordss.append(sub_to_words)\n",
      "   116                                           \n",
      "   117                                                   # relation prediction\n",
      "   118         5       4070.0    814.0      0.0          if rel_out != None:\n",
      "   119         5       4101.0    820.2      0.0              pairs_idx = rel_out['pairs_idx']\n",
      "   120         5       2791.0    558.2      0.0              rel_pred = rel_out['preds']\n",
      "   121                                           \n",
      "   122         5       7688.0   1537.6      0.0              rel_ann = {'pairs_idx': pairs_idx, 'rel_preds': rel_pred}\n",
      "   123         5       3309.0    661.8      0.0              rel_anns.append(rel_ann)\n",
      "   124                                                   else:\n",
      "   125                                                       rel_anns.append({})\n",
      "   126                                           \n",
      "   127                                                   # event prediction\n",
      "   128         5       2031.0    406.2      0.0          if ev_out != None:\n",
      "   129                                                       # add predicted entity\n",
      "   130         5       4612.0    922.4      0.0              ent_preds.append(ner_out[\"nner_preds\"])\n",
      "   131                                           \n",
      "   132                                                       # add predicted events\n",
      "   133         5       3160.0    632.0      0.0              ev_preds.append(ev_out)\n",
      "   134                                           \n",
      "   135         5       3629.0    725.8      0.0              span_indicess.append(\n",
      "   136         5   13074820.0 2614964.0      0.0                  [\n",
      "   137                                                               indice.detach().cpu().numpy()\n",
      "   138         5       1741.0    348.2      0.0                      for indice in ner_out[\"span_indices\"]\n",
      "   139                                                           ]\n",
      "   140                                                       )\n",
      "   141         5       3230.0    646.0      0.0              is_eval_ev = True\n",
      "   142                                                   else:\n",
      "   143                                                       ent_preds.append([])\n",
      "   144                                                       ev_preds.append([])\n",
      "   145                                           \n",
      "   146                                                       span_indicess.append([])\n",
      "   147                                           \n",
      "   148                                                   # Clear GPU unused RAM:\n",
      "   149                                                   # if params['gpu'] >= 0:\n",
      "   150                                                   #     torch.cuda.empty_cache()\n",
      "   151                                           \n",
      "   152         5     406996.0  81399.2      0.0          print(\"PREDICT LOOP: --- %s seconds ---\" % (time.time() - start_time))\n",
      "   153                                           \n",
      "   154         1       1311.0   1311.0      0.0      file_time = time.time()        \n",
      "   155                                               # write entity and relation prediction\n",
      "   156         1 6921201051.0 6921201051.0     14.6      _ = write_entity_relations(\n",
      "   157         1        291.0    291.0      0.0          result_dir=result_dir,\n",
      "   158         1        280.0    280.0      0.0          fidss=fidss,\n",
      "   159         1        300.0    300.0      0.0          ent_anns=ent_anns,\n",
      "   160         1        240.0    240.0      0.0          rel_anns=rel_anns,\n",
      "   161         1        240.0    240.0      0.0          params=params\n",
      "   162                                               )\n",
      "   163                                           \n",
      "   164         1       1129.0   1129.0      0.0      if is_eval_ev > 0:\n",
      "   165         1 1029615006.0 1029615006.0      2.2          write_events(fids=fidss,\n",
      "   166         1        511.0    511.0      0.0                       all_ent_preds=ent_preds,\n",
      "   167         1        520.0    520.0      0.0                       all_words=wordss,\n",
      "   168         1        491.0    491.0      0.0                       all_offsets=offsetss,\n",
      "   169         1        630.0    630.0      0.0                       all_span_terms=all_ner_terms,\n",
      "   170         1        510.0    510.0      0.0                       all_span_indices=span_indicess,\n",
      "   171         1        930.0    930.0      0.0                       all_sub_to_words=sub_to_wordss,\n",
      "   172         1        540.0    540.0      0.0                       all_ev_preds=ev_preds,\n",
      "   173         1        520.0    520.0      0.0                       g_entity_ids_=g_entity_ids_,\n",
      "   174         1        250.0    250.0      0.0                       params=params,\n",
      "   175         1        260.0    260.0      0.0                       result_dir=result_dir)\n",
      "   176                                           \n",
      "   177         1     150269.0 150269.0      0.0      print(\"(FILE writing): --- %s seconds ---\" % (time.time() - file_time))"
     ]
    }
   ],
   "source": [
    "%lprun -f predict predict(model=deepee_model, result_dir=result_dir, eval_dataloader=test_dataloader, eval_data=nntest_data, g_entity_ids_=test_data['g_entity_ids_'], params=parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_preds.pickle','rb') as handle:\n",
    "    all_preds = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [12,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0]], dtype=uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds[365:370]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trigger_indices.pickle', 'rb') as handle:\n",
    "    trigger_indices = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[367, 660, 881, 1500, 1573, 3960, 7851, 7852, 7865, 8768]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigger_indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trigger_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.from_numpy(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28845, 2])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20942,     0],\n",
       "        [21641,     0],\n",
       "        [22111,     0],\n",
       "        [22441,     0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds == 1).nonzero(as_tuple = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = torch.tensor(range(1,41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "        19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "        37, 38, 39, 40])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index2= torch.tensor([[0],[0]])\n",
    "index0= torch.zeros([preds.shape[0],1],dtype=int)\n",
    "index1= torch.ones([preds.shape[0],1],dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.gather(1,index0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigers0 = (preds.gather(1, index0) == indx).nonzero(as_tuple=False)\n",
    "trigers1 = (preds.gather(1, index1) == indx).nonzero(as_tuple=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[27172,     8]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigers1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  367,    11],\n",
       "        [  660,    22],\n",
       "        [  881,    22],\n",
       "        [ 1500,    11],\n",
       "        [ 1573,    11],\n",
       "        [ 3960,    11],\n",
       "        [ 7851,     9],\n",
       "        [ 7852,     8],\n",
       "        [ 7865,     8],\n",
       "        [ 8768,     9],\n",
       "        [ 9454,     2],\n",
       "        [11612,    11],\n",
       "        [13615,     2],\n",
       "        [16976,    11],\n",
       "        [17512,    11],\n",
       "        [18313,    15],\n",
       "        [19750,    11],\n",
       "        [19813,    13],\n",
       "        [20336,    11],\n",
       "        [20897,    22],\n",
       "        [20942,     0],\n",
       "        [20970,     2],\n",
       "        [21614,     8],\n",
       "        [21641,     0],\n",
       "        [22111,     0],\n",
       "        [22125,     8],\n",
       "        [22377,     4],\n",
       "        [22441,     0],\n",
       "        [22460,     8],\n",
       "        [22566,     4],\n",
       "        [22720,     4],\n",
       "        [22776,     2],\n",
       "        [22957,    23],\n",
       "        [23021,     8],\n",
       "        [23280,     5],\n",
       "        [23350,     4],\n",
       "        [23448,     8],\n",
       "        [23588,     4],\n",
       "        [23644,     4],\n",
       "        [23713,    11],\n",
       "        [23791,     4],\n",
       "        [23847,     6],\n",
       "        [24414,     8],\n",
       "        [24603,     2],\n",
       "        [24676,     8],\n",
       "        [24860,    11],\n",
       "        [24863,    11],\n",
       "        [24870,    11],\n",
       "        [25392,    11],\n",
       "        [25423,    11],\n",
       "        [25773,    10],\n",
       "        [25870,    11],\n",
       "        [26144,    10],\n",
       "        [26255,    11],\n",
       "        [26258,    11],\n",
       "        [26284,    11],\n",
       "        [27172,    11],\n",
       "        [27571,    10],\n",
       "        [27599,     2],\n",
       "        [28506,    11],\n",
       "        [28839,    10]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigers0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trigers0= trigers0.gather(1,\n",
    "                torch.zeros([trigers0.shape[0], 1], dtype=int)\n",
    "                ).squeeze(1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigers1 = trigers1.gather(1,\n",
    "                torch.zeros([trigers1.shape[0], 1], dtype=int)\n",
    "                ).squeeze(1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigers = list(set(trigers0 + trigers1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27172,\n",
       " 18313,\n",
       " 21641,\n",
       " 25870,\n",
       " 26255,\n",
       " 26258,\n",
       " 660,\n",
       " 23448,\n",
       " 24603,\n",
       " 24860,\n",
       " 24863,\n",
       " 22776,\n",
       " 20897,\n",
       " 23713,\n",
       " 26144,\n",
       " 23588,\n",
       " 1573,\n",
       " 19750,\n",
       " 22566,\n",
       " 23847,\n",
       " 22441,\n",
       " 24870,\n",
       " 7851,\n",
       " 7852,\n",
       " 22957,\n",
       " 25773,\n",
       " 13615,\n",
       " 25392,\n",
       " 26284,\n",
       " 27571,\n",
       " 23350,\n",
       " 7865,\n",
       " 22460,\n",
       " 8768,\n",
       " 22720,\n",
       " 28839,\n",
       " 20942,\n",
       " 25423,\n",
       " 16976,\n",
       " 27599,\n",
       " 28506,\n",
       " 11612,\n",
       " 1500,\n",
       " 23644,\n",
       " 22111,\n",
       " 24414,\n",
       " 24676,\n",
       " 19813,\n",
       " 17512,\n",
       " 22377,\n",
       " 20970,\n",
       " 22125,\n",
       " 9454,\n",
       " 367,\n",
       " 20336,\n",
       " 881,\n",
       " 21614,\n",
       " 23021,\n",
       " 23280,\n",
       " 23791,\n",
       " 3960]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('ontonerd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c215c50f3ee9fb94f7abb07f40a8d094769bf1b1c1172e473819ed198c6051f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
