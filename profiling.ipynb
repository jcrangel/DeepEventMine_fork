{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext memory_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "from eval.evaluate import predict\n",
    "\n",
    "from nets import deepEM\n",
    "from loader.prepData import prepdata\n",
    "from loader.prepNN import prep4nn\n",
    "from loader.prepNN.prep4nn import torch_data_2_network\n",
    "from utils import utils\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(obj, seen=None):\n",
    "    \"\"\"Recursively finds size of objects\"\"\"\n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    # Important mark as seen *before* entering recursion to gracefully handle\n",
    "    # self-referential objects\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "\n",
    "    return size / (1024*1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_test_data(test_data, params):\n",
    "\n",
    "    test = prep4nn.data2network(test_data, 'predict', params)\n",
    "\n",
    "    if len(test) == 0:\n",
    "        raise ValueError(\"Test set empty.\")\n",
    "\n",
    "    #VERY slow\n",
    "    nntest_data = prep4nn.torch_data_2_network(\n",
    "        cdata2network=test, params=params, do_get_nn_data=True)\n",
    "\n",
    "    te_data_size = len(nntest_data['nn_data']['ids'])\n",
    "\n",
    "    test_data_ids = TensorDataset(torch.arange(te_data_size))\n",
    "\n",
    "    test_sampler = SequentialSampler(test_data_ids)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data_ids, sampler=test_sampler, batch_size=params['batchsize'])\n",
    "    return nntest_data, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 09:17:07.924 | INFO     | bert.tokenization:from_pretrained:171 - loading vocabulary file data/bert/scibert_scivocab_cased/vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tWords found in train: 10876\n",
      "\tWords found in pre-trained only: 0\n",
      "\tWords not found anywhere: 2083\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# inp_args = utils._parsing_jupyter()\n",
    "config_path = '/home/julio/repos/event_finder/DeepEventMine_fork/experiments/pubmed100/configs/predict-pubmed-100.yaml'\n",
    "\n",
    "# set config path manually\n",
    "# config_path = 'configs/debug.yaml'\n",
    "\n",
    "with open(config_path, 'r') as stream:\n",
    "    pred_params = utils._ordered_load(stream)\n",
    "\n",
    "# Fix seed for reproducibility\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(pred_params['seed'])\n",
    "random.seed(pred_params['seed'])\n",
    "np.random.seed(pred_params['seed'])\n",
    "torch.manual_seed(pred_params['seed'])\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Load pre-trained parameters\n",
    "with open(pred_params['saved_params'], \"rb\") as f:\n",
    "    parameters = pickle.load(f)\n",
    "\n",
    "parameters['predict'] = True\n",
    "\n",
    "# Set predict settings value for params\n",
    "parameters['gpu'] = pred_params['gpu']\n",
    "parameters['batchsize'] = pred_params['batchsize']\n",
    "# print('GPU available:', torch.cuda.is_available())\n",
    "if parameters['gpu'] >= 0:\n",
    "    device = torch.device(\n",
    "        \"cuda:\" + str(parameters['gpu']) if torch.cuda.is_available() else \"cpu\")\n",
    "    # torch.cuda.set_device(parameters['gpu'])\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "parameters['device'] = device\n",
    "\n",
    "# Set evaluation settings\n",
    "parameters['test_data'] = pred_params['test_data']\n",
    "\n",
    "parameters['bert_model'] = pred_params['bert_model']\n",
    "\n",
    "result_dir = pred_params['result_dir']\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "parameters['result_dir'] = pred_params['result_dir']\n",
    "\n",
    "# raw text\n",
    "parameters['raw_text'] = pred_params['raw_text']\n",
    "parameters['ner_predict_all'] = pred_params['raw_text']\n",
    "parameters['a2_entities'] = pred_params['a2_entities']\n",
    "parameters['json_file'] = pred_params['json_file']\n",
    "\n",
    "# process data\n",
    "test_data = prepdata.prep_input_data(\n",
    "    pred_params['test_data'], parameters, json_file=parameters['json_file'])\n",
    "# nntest_data, test_dataloader = read_test_data(test_data, parameters)\n",
    "test = prep4nn.data2network(test_data, 'predict', parameters)\n",
    "\n",
    "if len(test) == 0:\n",
    "    raise ValueError(\"Test set empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 09:19:33.159 | INFO     | bert.tokenization:from_pretrained:171 - loading vocabulary file data/bert/scibert_scivocab_cased/vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 14.4106 s\n",
      "File: /home/julio/repos/event_finder/DeepEventMine_fork/loader/prepNN/prep4nn.py\n",
      "Function: torch_data_2_network at line 69\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    69                                           def torch_data_2_network(cdata2network, params, do_get_nn_data):\n",
      "    70                                               \"\"\" Convert object-type data to torch.tensor type data, aim to use with Pytorch\n",
      "    71                                               \"\"\"\n",
      "    72         1     295199.0 295199.0      0.0      etypes = [data['etypes2'] for data in cdata2network]\n",
      "    73                                           \n",
      "    74                                               # nner\n",
      "    75         1      97029.0  97029.0      0.0      entitiess = [data['entities'] for data in cdata2network]\n",
      "    76         1     119470.0 119470.0      0.0      sw_sentences = [data['sw_sentence'] for data in cdata2network]\n",
      "    77         1      64290.0  64290.0      0.0      termss = [data['terms'] for data in cdata2network]\n",
      "    78         1      85629.0  85629.0      0.0      valid_startss = [data['valid_starts'] for data in cdata2network]\n",
      "    79                                           \n",
      "    80         1     106879.0 106879.0      0.0      fids = [data['fid'] for data in cdata2network]\n",
      "    81         1     135519.0 135519.0      0.0      wordss = [data['words'] for data in cdata2network]\n",
      "    82         1      89650.0  89650.0      0.0      offsetss = [data['offsets'] for data in cdata2network]\n",
      "    83         1      97260.0  97260.0      0.0      sub_to_words = [data['sub_to_word'] for data in cdata2network]\n",
      "    84         1     106650.0 106650.0      0.0      subwords = [data['subwords'] for data in cdata2network]\n",
      "    85                                           \n",
      "    86         1   29799896.0 29799896.0      0.2      tokenizer = BertTokenizer.from_pretrained(\n",
      "    87         1       1100.0   1100.0      0.0          params['bert_model'], do_lower_case=False\n",
      "    88                                               )\n",
      "    89                                           \n",
      "    90                                               # User-defined data\n",
      "    91         1       1380.0   1380.0      0.0      if not params[\"predict\"]:\n",
      "    92                                                   id_tag_mapping = params[\"mappings\"][\"nn_mapping\"][\"id_tag_mapping\"]\n",
      "    93                                           \n",
      "    94                                                   mlb = MultiLabelBinarizer()\n",
      "    95                                                   mlb.fit([sorted(id_tag_mapping)[1:]])  # [1:] skip label O\n",
      "    96                                           \n",
      "    97                                                   params[\"mappings\"][\"nn_mapping\"][\"mlb\"] = mlb\n",
      "    98                                                   params[\"mappings\"][\"nn_mapping\"][\"num_labels\"] = len(mlb.classes_)\n",
      "    99                                           \n",
      "   100                                                   params[\"max_span_width\"] = max(params[\"max_entity_width\"], params[\"max_trigger_width\"])\n",
      "   101                                           \n",
      "   102                                                   params[\"mappings\"][\"nn_mapping\"][\"num_triggers\"] = len(params[\"mappings\"][\"nn_mapping\"][\"trigger_labels\"])\n",
      "   103                                                   params[\"mappings\"][\"nn_mapping\"][\"num_entities\"] = params[\"mappings\"][\"nn_mapping\"][\"num_labels\"] - \\\n",
      "   104                                                                                                      params[\"mappings\"][\"nn_mapping\"][\"num_triggers\"]\n",
      "   105                                           \n",
      "   106         1        210.0    210.0      0.0      if do_get_nn_data:\n",
      "   107         1 14379589446.0 14379589446.0     99.8          nn_data = get_nn_data(fids, entitiess, termss, valid_startss, sw_sentences,\n",
      "   108         1        180.0    180.0      0.0                                tokenizer, params)\n",
      "   109                                           \n",
      "   110         1       2029.0   2029.0      0.0          return {'nn_data': nn_data, 'etypes': etypes, 'fids': fids, 'words': wordss, 'offsets': offsetss,\n",
      "   111         1        220.0    220.0      0.0                  'sub_to_words': sub_to_words, 'subwords': subwords, 'entities': entitiess}"
     ]
    }
   ],
   "source": [
    "\n",
    "#VERY slow\n",
    "# nntest_data = prep4nn.torch_data_2_network(\n",
    "#     cdata2network=test, params=parameters, do_get_nn_data=True)\n",
    "%lprun -f torch_data_2_network torch_data_2_network(cdata2network=test, params=parameters, do_get_nn_data=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "te_data_size = len(nntest_data['nn_data']['ids'])\n",
    "\n",
    "test_data_ids = TensorDataset(torch.arange(te_data_size))\n",
    "\n",
    "test_sampler = SequentialSampler(test_data_ids)\n",
    "test_dataloader = DataLoader(\n",
    "    test_data_ids, sampler=test_sampler, batch_size=parameters['batchsize'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f datastuff datastuff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from predict_no_files import read_test_data as read_test_data2\n",
    "# %mprun -f read_test_data2 read_test_data2(test_data, parameters)\n",
    "\n",
    "# %mprun -f read_test_data2 read_test_data2(test_data, parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 09:12:24.902 | INFO     | bert.tokenization:from_pretrained:171 - loading vocabulary file data/bert/scibert_scivocab_cased/vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tWords found in train: 10876\n",
      "\tWords found in pre-trained only: 0\n",
      "\tWords not found anywhere: 2083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 09:12:25.903 | INFO     | bert.tokenization:from_pretrained:171 - loading vocabulary file data/bert/scibert_scivocab_cased/vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 15.5915 s\n",
      "File: /tmp/ipykernel_31242/2669338896.py\n",
      "Function: read_test_data at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def read_test_data(test_data, params):\n",
      "     2                                           \n",
      "     3         1 1015019737.0 1015019737.0      6.5      test = prep4nn.data2network(test_data, 'predict', params)\n",
      "     4                                           \n",
      "     5         1       1950.0   1950.0      0.0      if len(test) == 0:\n",
      "     6                                                   raise ValueError(\"Test set empty.\")\n",
      "     7                                           \n",
      "     8         1 14576032129.0 14576032129.0     93.5      test_data = prep4nn.torch_data_2_network(\n",
      "     9         1        260.0    260.0      0.0          cdata2network=test, params=params, do_get_nn_data=True)\n",
      "    10                                           \n",
      "    11         1       8430.0   8430.0      0.0      te_data_size = len(test_data['nn_data']['ids'])\n",
      "    12                                           \n",
      "    13         1     177199.0 177199.0      0.0      test_data_ids = TensorDataset(torch.arange(te_data_size))\n",
      "    14                                           \n",
      "    15         1      10250.0  10250.0      0.0      test_sampler = SequentialSampler(test_data_ids)\n",
      "    16         1     200998.0 200998.0      0.0      test_dataloader = DataLoader(\n",
      "    17         1       2100.0   2100.0      0.0          test_data_ids, sampler=test_sampler, batch_size=params['batchsize'])\n",
      "    18         1        180.0    180.0      0.0      return test_data, test_dataloader"
     ]
    }
   ],
   "source": [
    "# %lprun -f read_test_data read_test_data(test_data, parameters)\n",
    "# nntest_data, test_dataloader = read_test_data(test_data, parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model\n",
    "deepee_model = deepEM.DeepEM(parameters)\n",
    "\n",
    "model_path = pred_params['model_path']\n",
    "\n",
    "# Load all models\n",
    "utils.handle_checkpoints(model=deepee_model,\n",
    "                            checkpoint_dir=model_path,\n",
    "                            params={\n",
    "                                'device': device\n",
    "                            },\n",
    "                            resume=True)\n",
    "\n",
    "deepee_model.to(device)\n",
    "\n",
    "# with profile(activities=[\n",
    "#         ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True,with_stack=True) as prof:\n",
    "#     with record_function(\"model_inference\"):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER LOOP: --- 0.0008637905120849609 seconds ---\n",
      "NER LAYER: --- 0.09203028678894043 seconds ---\n",
      "REL LAYER: --- 0.01859426498413086 seconds ---\n",
      "EV LAYER: --- 0.04557323455810547 seconds ---\n",
      "ALL FOWARD LAYER: --- 0.32856082916259766 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  20%|██        | 1/5 [00:01<00:04,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 1.2396204471588135 seconds ---\n",
      "NER LOOP: --- 0.0009527206420898438 seconds ---\n",
      "NER LAYER: --- 0.10408425331115723 seconds ---\n",
      "REL LAYER: --- 0.022671937942504883 seconds ---\n",
      "EV LAYER: --- 0.14000391960144043 seconds ---\n",
      "ALL FOWARD LAYER: --- 0.552802562713623 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  40%|████      | 2/5 [00:03<00:04,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 1.8513519763946533 seconds ---\n",
      "NER LOOP: --- 0.0010538101196289062 seconds ---\n",
      "NER LAYER: --- 0.10121560096740723 seconds ---\n",
      "REL LAYER: --- 0.019904613494873047 seconds ---\n",
      "EV LAYER: --- 0.11380362510681152 seconds ---\n",
      "ALL FOWARD LAYER: --- 0.5128979682922363 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  60%|██████    | 3/5 [00:04<00:03,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 1.6618950366973877 seconds ---\n",
      "NER LOOP: --- 0.0009343624114990234 seconds ---\n",
      "NER LAYER: --- 0.09099411964416504 seconds ---\n",
      "REL LAYER: --- 0.019932270050048828 seconds ---\n",
      "EV LAYER: --- 0.09959864616394043 seconds ---\n",
      "ALL FOWARD LAYER: --- 0.46517395973205566 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 1.5450270175933838 seconds ---\n",
      "NER LOOP: --- 0.00026345252990722656 seconds ---\n",
      "NER LAYER: --- 0.014961957931518555 seconds ---\n",
      "REL LAYER: --- 0.006133317947387695 seconds ---\n",
      "EV LAYER: --- 0.006146669387817383 seconds ---\n",
      "ALL FOWARD LAYER: --- 0.04220175743103027 seconds ---\n",
      "PREDICT LOOP: --- 0.0813448429107666 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(FILE writing): --- 7.6365134716033936 seconds ---\n",
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 13.7251 s\n",
      "File: /home/julio/repos/event_finder/DeepEventMine_fork/eval/evaluate.py\n",
      "Function: predict at line 9\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     9                                           def predict(model, result_dir, eval_dataloader, eval_data, g_entity_ids_, params):\n",
      "    10         1       3880.0   3880.0      0.0      mapping_id_tag = params['mappings']['nn_mapping']['id_tag_mapping']\n",
      "    11                                           \n",
      "    12                                               # store predicted entities\n",
      "    13         1        760.0    760.0      0.0      ent_preds = []\n",
      "    14                                           \n",
      "    15                                               # store predicted events\n",
      "    16         1        400.0    400.0      0.0      ev_preds = []\n",
      "    17                                           \n",
      "    18         1       1000.0   1000.0      0.0      fidss, wordss, offsetss, sub_to_wordss, span_indicess = [], [], [], [], []\n",
      "    19                                           \n",
      "    20                                               # entity and relation output\n",
      "    21         1        520.0    520.0      0.0      ent_anns = []\n",
      "    22         1        460.0    460.0      0.0      rel_anns = []\n",
      "    23                                           \n",
      "    24                                               # Evaluation phase\n",
      "    25         1    1715386.0 1715386.0      0.0      model.eval()\n",
      "    26                                           \n",
      "    27         1        410.0    410.0      0.0      all_ner_preds, all_ner_golds, all_ner_terms = [], [], []\n",
      "    28                                           \n",
      "    29         1        140.0    140.0      0.0      is_eval_ev = False\n",
      "    30         5    9972998.0 1994599.6      0.1      for step, batch in enumerate(\n",
      "    31         1     844003.0 844003.0      0.0              tqdm(eval_dataloader, desc=\"Iteration\", leave=False)\n",
      "    32                                               ):\n",
      "    33         5       5140.0   1028.0      0.0          start_time = time.time()\n",
      "    34         5       6730.0   1346.0      0.0          eval_data_ids = batch\n",
      "    35         5 3003880402.0 600776080.4     21.9          tensors = utils.get_tensors(eval_data_ids, eval_data, params)\n",
      "    36                                           \n",
      "    37         5     117760.0  23552.0      0.0          nn_tokens, nn_ids, nn_token_mask, nn_attention_mask, nn_span_indices, nn_span_labels, nn_span_labels_match_rel, nn_entity_masks, nn_trigger_masks, _, \\\n",
      "    38         5      17430.0   3486.0      0.0          etypes, _ = tensors\n",
      "    39                                           \n",
      "    40         5     283607.0  56721.4      0.0          fids = [\n",
      "    41         5      38470.0   7694.0      0.0              eval_data[\"fids\"][data_id] for data_id in eval_data_ids[0].tolist()\n",
      "    42                                                   ]\n",
      "    43         5     446336.0  89267.2      0.0          offsets = [\n",
      "    44                                                       eval_data[\"offsets\"][data_id]\n",
      "    45         5      10080.0   2016.0      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    46                                                   ]\n",
      "    47         5     161649.0  32329.8      0.0          words = [\n",
      "    48         5       8150.0   1630.0      0.0              eval_data[\"words\"][data_id] for data_id in eval_data_ids[0].tolist()\n",
      "    49                                                   ]\n",
      "    50         5     186371.0  37274.2      0.0          sub_to_words = [\n",
      "    51                                                       eval_data[\"sub_to_words\"][data_id]\n",
      "    52         5       7930.0   1586.0      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    53                                                   ]\n",
      "    54         5     229857.0  45971.4      0.0          subwords = [\n",
      "    55                                                       eval_data[\"subwords\"][data_id]\n",
      "    56         5       8032.0   1606.4      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    57                                                   ]\n",
      "    58         5     149040.0  29808.0      0.0          gold_entities = [\n",
      "    59                                                       eval_data[\"entities\"][data_id]\n",
      "    60         5       8091.0   1618.2      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    61                                                   ]\n",
      "    62                                           \n",
      "    63         5     169648.0  33929.6      0.0          with torch.no_grad():\n",
      "    64         5 1902212164.0 380442432.8     13.9              ner_out, rel_out, ev_out = model(tensors, params)\n",
      "    65                                           \n",
      "    66         5       2701.0    540.2      0.0          ner_preds = ner_out['preds']\n",
      "    67                                           \n",
      "    68         5       1479.0    295.8      0.0          ner_terms = ner_out['terms']\n",
      "    69                                           \n",
      "    70         5       3980.0    796.0      0.0          all_ner_terms.append(ner_terms)\n",
      "    71                                           \n",
      "    72       516     253988.0    492.2      0.0          for sentence_idx, ner_pred in enumerate(ner_preds):\n",
      "    73       516     205119.0    397.5      0.0              all_ner_golds.append(\n",
      "    74       516     570360.0   1105.3      0.0                  [\n",
      "    75                                                               (\n",
      "    76                                                                   sub_to_words[sentence_idx][span_start],\n",
      "    77                                                                   sub_to_words[sentence_idx][span_end],\n",
      "    78                                                                   mapping_id_tag[label_id],\n",
      "    79                                                               )\n",
      "    80                                                               for (\n",
      "    81                                                                       span_start,\n",
      "    82                                                                       span_end,\n",
      "    83       516     315799.0    612.0      0.0                          ), label_ids in gold_entities[sentence_idx].items()\n",
      "    84                                                               for label_id in label_ids\n",
      "    85                                                           ]\n",
      "    86                                                       )\n",
      "    87       516  235453288.0 456304.8      1.7              nn_span = nn_span_indices.detach().cpu().numpy()\n",
      "    88       516     241021.0    467.1      0.0              pred_entities = []  # nn_span_indice :orch.Size([128, 2450, 2]), sentence_idx:0,span_id:0\n",
      "    89    331296  114789565.0    346.5      0.8              for span_id, ner_pred_id in enumerate(ner_pred):\n",
      "    90    331296  524550124.0   1583.3      3.8                  span_start, span_end = nn_span[sentence_idx][span_id]\n",
      "    91                                                           # indx = torch.tensor([sentence_idx, span_id],\n",
      "    92                                                           #                     device=0).unsqueeze(-1)\n",
      "    93                                                           # span_start, span_end = torch.gather(nn_span_indices,1,indx)\n",
      "    94                                           \n",
      "    95                                                           # span_start, span_end = span_start.item(), span_end.item()\n",
      "    96    329435  156564027.0    475.3      1.1                  if (ner_pred_id > 0\n",
      "    97      1861    1422736.0    764.5      0.0                          and span_start in sub_to_words[sentence_idx]\n",
      "    98      1861     861137.0    462.7      0.0                          and span_end in sub_to_words[sentence_idx]\n",
      "    99                                                           ):\n",
      "   100      1861     686797.0    369.0      0.0                      pred_entities.append(\n",
      "   101      1861     745599.0    400.6      0.0                          (\n",
      "   102      1861     992771.0    533.5      0.0                              sub_to_words[sentence_idx][span_start],\n",
      "   103      1861     880108.0    472.9      0.0                              sub_to_words[sentence_idx][span_end],\n",
      "   104      1861     861957.0    463.2      0.0                              mapping_id_tag[ner_pred_id],\n",
      "   105                                                                   )\n",
      "   106                                                               )\n",
      "   107       516     249673.0    483.9      0.0              all_ner_preds.append(pred_entities)\n",
      "   108                                           \n",
      "   109                                                   # entity prediction\n",
      "   110         5       9340.0   1868.0      0.0          ent_ann = {'span_indices': nn_span_indices, 'ner_preds': ner_out['preds'], 'words': words,\n",
      "   111         5       2599.0    519.8      0.0                     'offsets': offsets, 'sub_to_words': sub_to_words, 'subwords': subwords,\n",
      "   112         5       1320.0    264.0      0.0                     'ner_terms': ner_terms}\n",
      "   113         5       3431.0    686.2      0.0          ent_anns.append(ent_ann)\n",
      "   114                                           \n",
      "   115         5       5231.0   1046.2      0.0          fidss.append(fids)\n",
      "   116                                           \n",
      "   117         5       3900.0    780.0      0.0          wordss.append(words)\n",
      "   118         5       4080.0    816.0      0.0          offsetss.append(offsets)\n",
      "   119         5       4080.0    816.0      0.0          sub_to_wordss.append(sub_to_words)\n",
      "   120                                           \n",
      "   121                                                   # relation prediction\n",
      "   122         5       5360.0   1072.0      0.0          if rel_out != None:\n",
      "   123         5       4448.0    889.6      0.0              pairs_idx = rel_out['pairs_idx']\n",
      "   124         5       3050.0    610.0      0.0              rel_pred = rel_out['preds']\n",
      "   125                                           \n",
      "   126         5       6970.0   1394.0      0.0              rel_ann = {'pairs_idx': pairs_idx, 'rel_preds': rel_pred}\n",
      "   127         5       2619.0    523.8      0.0              rel_anns.append(rel_ann)\n",
      "   128                                                   else:\n",
      "   129                                                       rel_anns.append({})\n",
      "   130                                           \n",
      "   131                                                   # event prediction\n",
      "   132         5       1860.0    372.0      0.0          if ev_out != None:\n",
      "   133                                                       # add predicted entity\n",
      "   134         5       3990.0    798.0      0.0              ent_preds.append(ner_out[\"nner_preds\"])\n",
      "   135                                           \n",
      "   136                                                       # add predicted events\n",
      "   137         5       4350.0    870.0      0.0              ev_preds.append(ev_out)\n",
      "   138                                           \n",
      "   139         5       3451.0    690.2      0.0              span_indicess.append(\n",
      "   140         5   12773814.0 2554762.8      0.1                  [\n",
      "   141                                                               indice.detach().cpu().numpy()\n",
      "   142         5       1569.0    313.8      0.0                      for indice in ner_out[\"span_indices\"]\n",
      "   143                                                           ]\n",
      "   144                                                       )\n",
      "   145         5       1990.0    398.0      0.0              is_eval_ev = True\n",
      "   146                                                   else:\n",
      "   147                                                       ent_preds.append([])\n",
      "   148                                                       ev_preds.append([])\n",
      "   149                                           \n",
      "   150                                                       span_indicess.append([])\n",
      "   151                                           \n",
      "   152                                                   # Clear GPU unused RAM:\n",
      "   153         5       8120.0   1624.0      0.0          if params['gpu'] >= 0:\n",
      "   154         5  115207201.0 23041440.2      0.8              torch.cuda.empty_cache()\n",
      "   155                                           \n",
      "   156         5     284508.0  56901.6      0.0          print(\"PREDICT LOOP: --- %s seconds ---\" % (time.time() - start_time))\n",
      "   157                                           \n",
      "   158         1        710.0    710.0      0.0      file_time = time.time()        \n",
      "   159                                               # write entity and relation prediction\n",
      "   160         1 6743704488.0 6743704488.0     49.1      _ = write_entity_relations(\n",
      "   161         1        250.0    250.0      0.0          result_dir=result_dir,\n",
      "   162         1        230.0    230.0      0.0          fidss=fidss,\n",
      "   163         1        220.0    220.0      0.0          ent_anns=ent_anns,\n",
      "   164         1        220.0    220.0      0.0          rel_anns=rel_anns,\n",
      "   165         1        230.0    230.0      0.0          params=params\n",
      "   166                                               )\n",
      "   167                                           \n",
      "   168         1       1710.0   1710.0      0.0      if is_eval_ev > 0:\n",
      "   169         1  892752397.0 892752397.0      6.5          write_events(fids=fidss,\n",
      "   170         1        510.0    510.0      0.0                       all_ent_preds=ent_preds,\n",
      "   171         1        510.0    510.0      0.0                       all_words=wordss,\n",
      "   172         1        540.0    540.0      0.0                       all_offsets=offsetss,\n",
      "   173         1        400.0    400.0      0.0                       all_span_terms=all_ner_terms,\n",
      "   174         1        250.0    250.0      0.0                       all_span_indices=span_indicess,\n",
      "   175         1        300.0    300.0      0.0                       all_sub_to_words=sub_to_wordss,\n",
      "   176         1        320.0    320.0      0.0                       all_ev_preds=ev_preds,\n",
      "   177         1        390.0    390.0      0.0                       g_entity_ids_=g_entity_ids_,\n",
      "   178         1        220.0    220.0      0.0                       params=params,\n",
      "   179         1        250.0    250.0      0.0                       result_dir=result_dir)\n",
      "   180                                           \n",
      "   181         1     142519.0 142519.0      0.0      print(\"(FILE writing): --- %s seconds ---\" % (time.time() - file_time))"
     ]
    }
   ],
   "source": [
    "%lprun -f predict predict(model=deepee_model,result_dir=result_dir, eval_dataloader=test_dataloader,eval_data=nntest_data,g_entity_ids_=test_data['g_entity_ids_'],params=parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER LOOP: --- 4.088160753250122 seconds ---\n",
      "NER LAYER: --- 4.168576955795288 seconds ---\n",
      "REL LAYER: --- 0.01534891128540039 seconds ---\n",
      "EV LAYER: --- 0.04376649856567383 seconds ---\n",
      "ALL FOWARD LAYER: --- 4.417277812957764 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  20%|██        | 1/5 [00:07<00:28,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 7.059613227844238 seconds ---\n",
      "NER LOOP: --- 6.67397665977478 seconds ---\n",
      "NER LAYER: --- 6.782450199127197 seconds ---\n",
      "REL LAYER: --- 0.020621299743652344 seconds ---\n",
      "EV LAYER: --- 0.1400163173675537 seconds ---\n",
      "ALL FOWARD LAYER: --- 7.2468602657318115 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  40%|████      | 2/5 [00:18<00:28,  9.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 18.449894428253174 seconds ---\n",
      "NER LOOP: --- 6.6860737800598145 seconds ---\n",
      "NER LAYER: --- 6.782872915267944 seconds ---\n",
      "REL LAYER: --- 0.018086910247802734 seconds ---\n",
      "EV LAYER: --- 0.11701488494873047 seconds ---\n",
      "ALL FOWARD LAYER: --- 7.209285259246826 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  60%|██████    | 3/5 [00:29<00:20, 10.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 29.643948793411255 seconds ---\n",
      "NER LOOP: --- 5.759660959243774 seconds ---\n",
      "NER LAYER: --- 5.8500213623046875 seconds ---\n",
      "REL LAYER: --- 0.017380952835083008 seconds ---\n",
      "EV LAYER: --- 0.09950733184814453 seconds ---\n",
      "ALL FOWARD LAYER: --- 6.2324864864349365 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  80%|████████  | 4/5 [00:39<00:10, 10.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT LOOP: --- 39.43154978752136 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER LOOP: --- 0.2331676483154297 seconds ---\n",
      "NER LAYER: --- 0.24660420417785645 seconds ---\n",
      "REL LAYER: --- 0.01309823989868164 seconds ---\n",
      "EV LAYER: --- 0.009708166122436523 seconds ---\n",
      "ALL FOWARD LAYER: --- 0.2849557399749756 seconds ---\n",
      "PREDICT LOOP: --- 39.852705001831055 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(FILE writing): --- 7.950889587402344 seconds ---\n",
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 47.3856 s\n",
      "File: /home/julio/repos/event_finder/DeepEventMine_fork/eval/evaluate.py\n",
      "Function: predict at line 9\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     9                                           def predict(model, result_dir, eval_dataloader, eval_data, g_entity_ids_, params):\n",
      "    10         1       2630.0   2630.0      0.0      mapping_id_tag = params['mappings']['nn_mapping']['id_tag_mapping']\n",
      "    11                                           \n",
      "    12                                               # store predicted entities\n",
      "    13         1        220.0    220.0      0.0      ent_preds = []\n",
      "    14                                           \n",
      "    15                                               # store predicted events\n",
      "    16         1        170.0    170.0      0.0      ev_preds = []\n",
      "    17                                           \n",
      "    18         1        430.0    430.0      0.0      fidss, wordss, offsetss, sub_to_wordss, span_indicess = [], [], [], [], []\n",
      "    19                                           \n",
      "    20                                               # entity and relation output\n",
      "    21         1        180.0    180.0      0.0      ent_anns = []\n",
      "    22         1        150.0    150.0      0.0      rel_anns = []\n",
      "    23                                           \n",
      "    24                                               # Evaluation phase\n",
      "    25         1    1584648.0 1584648.0      0.0      model.eval()\n",
      "    26                                           \n",
      "    27         1        380.0    380.0      0.0      all_ner_preds, all_ner_golds, all_ner_terms = [], [], []\n",
      "    28                                           \n",
      "    29         1        140.0    140.0      0.0      is_eval_ev = False\n",
      "    30         1        650.0    650.0      0.0      start_time = time.time()\n",
      "    31         5    8868541.0 1773708.2      0.0      for step, batch in enumerate(\n",
      "    32         1    1533128.0 1533128.0      0.0              tqdm(eval_dataloader, desc=\"Iteration\", leave=False)\n",
      "    33                                               ):\n",
      "    34         5       9869.0   1973.8      0.0          eval_data_ids = batch\n",
      "    35         5 2925576103.0 585115220.6      6.2          tensors = utils.get_tensors(eval_data_ids, eval_data, params)\n",
      "    36                                           \n",
      "    37         5      97070.0  19414.0      0.0          nn_tokens, nn_ids, nn_token_mask, nn_attention_mask, nn_span_indices, nn_span_labels, nn_span_labels_match_rel, nn_entity_masks, nn_trigger_masks, _, \\\n",
      "    38         5      15270.0   3054.0      0.0          etypes, _ = tensors\n",
      "    39                                           \n",
      "    40         5     261317.0  52263.4      0.0          fids = [\n",
      "    41         5      44899.0   8979.8      0.0              eval_data[\"fids\"][data_id] for data_id in eval_data_ids[0].tolist()\n",
      "    42                                                   ]\n",
      "    43         5     220490.0  44098.0      0.0          offsets = [\n",
      "    44                                                       eval_data[\"offsets\"][data_id]\n",
      "    45         5       8960.0   1792.0      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    46                                                   ]\n",
      "    47         5     145028.0  29005.6      0.0          words = [\n",
      "    48         5       7790.0   1558.0      0.0              eval_data[\"words\"][data_id] for data_id in eval_data_ids[0].tolist()\n",
      "    49                                                   ]\n",
      "    50         5     174989.0  34997.8      0.0          sub_to_words = [\n",
      "    51                                                       eval_data[\"sub_to_words\"][data_id]\n",
      "    52         5       7970.0   1594.0      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    53                                                   ]\n",
      "    54         5     217237.0  43447.4      0.0          subwords = [\n",
      "    55                                                       eval_data[\"subwords\"][data_id]\n",
      "    56         5       7611.0   1522.2      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    57                                                   ]\n",
      "    58         5     127589.0  25517.8      0.0          gold_entities = [\n",
      "    59                                                       eval_data[\"entities\"][data_id]\n",
      "    60         5       7621.0   1524.2      0.0              for data_id in eval_data_ids[0].tolist()\n",
      "    61                                                   ]\n",
      "    62                                           \n",
      "    63         5     113351.0  22670.2      0.0          with torch.no_grad():\n",
      "    64         5 25391459058.0 5078291811.6     53.6              ner_out, rel_out, ev_out = model(tensors, params)\n",
      "    65                                           \n",
      "    66         5       3309.0    661.8      0.0          ner_preds = ner_out['preds']\n",
      "    67                                           \n",
      "    68         5       1288.0    257.6      0.0          ner_terms = ner_out['terms']\n",
      "    69                                           \n",
      "    70         5       4980.0    996.0      0.0          all_ner_terms.append(ner_terms)\n",
      "    71                                           \n",
      "    72       516     214539.0    415.8      0.0          for sentence_idx, ner_pred in enumerate(ner_preds):\n",
      "    73       516     262156.0    508.1      0.0              all_ner_golds.append(\n",
      "    74       516     601011.0   1164.8      0.0                  [\n",
      "    75                                                               (\n",
      "    76                                                                   sub_to_words[sentence_idx][span_start],\n",
      "    77                                                                   sub_to_words[sentence_idx][span_end],\n",
      "    78                                                                   mapping_id_tag[label_id],\n",
      "    79                                                               )\n",
      "    80                                                               for (\n",
      "    81                                                                       span_start,\n",
      "    82                                                                       span_end,\n",
      "    83       516     320289.0    620.7      0.0                          ), label_ids in gold_entities[sentence_idx].items()\n",
      "    84                                                               for label_id in label_ids\n",
      "    85                                                           ]\n",
      "    86                                                       )\n",
      "    87                                           \n",
      "    88       516     138847.0    269.1      0.0              pred_entities = []\n",
      "    89    331296  144147226.0    435.1      0.3              for span_id, ner_pred_id in enumerate(ner_pred):\n",
      "    90    331296 2209194422.0   6668.3      4.7                  span_start, span_end = nn_span_indices[sentence_idx][span_id]\n",
      "    91    331296 8546913531.0  25798.4     18.0                  span_start, span_end = span_start.item(), span_end.item()\n",
      "    92    329435  182987073.0    555.5      0.4                  if (ner_pred_id > 0\n",
      "    93      1861    1239498.0    666.0      0.0                          and span_start in sub_to_words[sentence_idx]\n",
      "    94      1861     513457.0    275.9      0.0                          and span_end in sub_to_words[sentence_idx]\n",
      "    95                                                           ):\n",
      "    96      1861     721754.0    387.8      0.0                      pred_entities.append(\n",
      "    97      1861     543594.0    292.1      0.0                          (\n",
      "    98      1861     693052.0    372.4      0.0                              sub_to_words[sentence_idx][span_start],\n",
      "    99      1861     552394.0    296.8      0.0                              sub_to_words[sentence_idx][span_end],\n",
      "   100      1861    1209304.0    649.8      0.0                              mapping_id_tag[ner_pred_id],\n",
      "   101                                                                   )\n",
      "   102                                                               )\n",
      "   103       516     266167.0    515.8      0.0              all_ner_preds.append(pred_entities)\n",
      "   104                                           \n",
      "   105                                                   # entity prediction\n",
      "   106         5      14298.0   2859.6      0.0          ent_ann = {'span_indices': nn_span_indices, 'ner_preds': ner_out['preds'], 'words': words,\n",
      "   107         5       2830.0    566.0      0.0                     'offsets': offsets, 'sub_to_words': sub_to_words, 'subwords': subwords,\n",
      "   108         5       1390.0    278.0      0.0                     'ner_terms': ner_terms}\n",
      "   109         5       4241.0    848.2      0.0          ent_anns.append(ent_ann)\n",
      "   110                                           \n",
      "   111         5       3901.0    780.2      0.0          fidss.append(fids)\n",
      "   112                                           \n",
      "   113         5       4391.0    878.2      0.0          wordss.append(words)\n",
      "   114         5       3131.0    626.2      0.0          offsetss.append(offsets)\n",
      "   115         5       2589.0    517.8      0.0          sub_to_wordss.append(sub_to_words)\n",
      "   116                                           \n",
      "   117                                                   # relation prediction\n",
      "   118         5       4070.0    814.0      0.0          if rel_out != None:\n",
      "   119         5       4101.0    820.2      0.0              pairs_idx = rel_out['pairs_idx']\n",
      "   120         5       2791.0    558.2      0.0              rel_pred = rel_out['preds']\n",
      "   121                                           \n",
      "   122         5       7688.0   1537.6      0.0              rel_ann = {'pairs_idx': pairs_idx, 'rel_preds': rel_pred}\n",
      "   123         5       3309.0    661.8      0.0              rel_anns.append(rel_ann)\n",
      "   124                                                   else:\n",
      "   125                                                       rel_anns.append({})\n",
      "   126                                           \n",
      "   127                                                   # event prediction\n",
      "   128         5       2031.0    406.2      0.0          if ev_out != None:\n",
      "   129                                                       # add predicted entity\n",
      "   130         5       4612.0    922.4      0.0              ent_preds.append(ner_out[\"nner_preds\"])\n",
      "   131                                           \n",
      "   132                                                       # add predicted events\n",
      "   133         5       3160.0    632.0      0.0              ev_preds.append(ev_out)\n",
      "   134                                           \n",
      "   135         5       3629.0    725.8      0.0              span_indicess.append(\n",
      "   136         5   13074820.0 2614964.0      0.0                  [\n",
      "   137                                                               indice.detach().cpu().numpy()\n",
      "   138         5       1741.0    348.2      0.0                      for indice in ner_out[\"span_indices\"]\n",
      "   139                                                           ]\n",
      "   140                                                       )\n",
      "   141         5       3230.0    646.0      0.0              is_eval_ev = True\n",
      "   142                                                   else:\n",
      "   143                                                       ent_preds.append([])\n",
      "   144                                                       ev_preds.append([])\n",
      "   145                                           \n",
      "   146                                                       span_indicess.append([])\n",
      "   147                                           \n",
      "   148                                                   # Clear GPU unused RAM:\n",
      "   149                                                   # if params['gpu'] >= 0:\n",
      "   150                                                   #     torch.cuda.empty_cache()\n",
      "   151                                           \n",
      "   152         5     406996.0  81399.2      0.0          print(\"PREDICT LOOP: --- %s seconds ---\" % (time.time() - start_time))\n",
      "   153                                           \n",
      "   154         1       1311.0   1311.0      0.0      file_time = time.time()        \n",
      "   155                                               # write entity and relation prediction\n",
      "   156         1 6921201051.0 6921201051.0     14.6      _ = write_entity_relations(\n",
      "   157         1        291.0    291.0      0.0          result_dir=result_dir,\n",
      "   158         1        280.0    280.0      0.0          fidss=fidss,\n",
      "   159         1        300.0    300.0      0.0          ent_anns=ent_anns,\n",
      "   160         1        240.0    240.0      0.0          rel_anns=rel_anns,\n",
      "   161         1        240.0    240.0      0.0          params=params\n",
      "   162                                               )\n",
      "   163                                           \n",
      "   164         1       1129.0   1129.0      0.0      if is_eval_ev > 0:\n",
      "   165         1 1029615006.0 1029615006.0      2.2          write_events(fids=fidss,\n",
      "   166         1        511.0    511.0      0.0                       all_ent_preds=ent_preds,\n",
      "   167         1        520.0    520.0      0.0                       all_words=wordss,\n",
      "   168         1        491.0    491.0      0.0                       all_offsets=offsetss,\n",
      "   169         1        630.0    630.0      0.0                       all_span_terms=all_ner_terms,\n",
      "   170         1        510.0    510.0      0.0                       all_span_indices=span_indicess,\n",
      "   171         1        930.0    930.0      0.0                       all_sub_to_words=sub_to_wordss,\n",
      "   172         1        540.0    540.0      0.0                       all_ev_preds=ev_preds,\n",
      "   173         1        520.0    520.0      0.0                       g_entity_ids_=g_entity_ids_,\n",
      "   174         1        250.0    250.0      0.0                       params=params,\n",
      "   175         1        260.0    260.0      0.0                       result_dir=result_dir)\n",
      "   176                                           \n",
      "   177         1     150269.0 150269.0      0.0      print(\"(FILE writing): --- %s seconds ---\" % (time.time() - file_time))"
     ]
    }
   ],
   "source": [
    "%lprun -f predict predict(model=deepee_model, result_dir=result_dir, eval_dataloader=test_dataloader, eval_data=nntest_data, g_entity_ids_=test_data['g_entity_ids_'], params=parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '/home/julio/repos/event_finder/data/pubmed/pubmed.json'\n",
    "\n",
    "with open(name,'r') as f:\n",
    "    pub = json.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49509365820"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_size(pub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47215.81060409546"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 49509365820\n",
    "size/(1024*1024)\n",
    "#~47 gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "backdict = {}\n",
    "for i,(k,v) in enumerate(pub.items()):\n",
    "    backdict[k] = v\n",
    "    if i == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/home/julio/repos/event_finder/data/pubmed/pubmed1000.json','w') as file:\n",
    "    json.dump(backdict,file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0c215c50f3ee9fb94f7abb07f40a8d094769bf1b1c1172e473819ed198c6051f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
